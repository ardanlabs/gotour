Profiling Code
We can use the go tooling to inspect and profile our programs. Profiling is more of a journey and detective work.

* Profiling Code

In this section, I will learn how to profile code using benchmarks. Even though I will be generating profiles from a benchmark, a lot of what is shared can be used regardless of how the profile is generated.

I can use the go tooling to inspect and profile my programs. Profiling is both a journey and detective work. It requires some understanding about the application and expectations. The profiling data in and of itself is just raw numbers. I have to give it meaning and understanding.

*The* *Basics* *of* *Profiling*

"Those who can make you believe absurdities can make you commit atrocities" - Voltaire

How does a profiler work?

- A profiler runs my program and configures the operating system to interrupt it at regular intervals. This is done by sending SIGPROF to the program being profiled, which suspends and transfers execution to the profiler. The profiler then grabs the program counter for each executing thread and then continues running the program.

*Profiling* *do's* *and* *don'ts*

Before I profile, I must have a stable environment to get repeatable results.

- The machine must be idle—don't profile on shared hardware, don't browse the web while waiting for a long benchmark to run.
- Watch out for power saving and thermal scaling.
- Avoid virtual machines and shared cloud hosting; they are too noisy for consistent measurements.

If I can afford it, buy dedicated performance test hardware. Rack them, disable all the power management and thermal scaling and never update the software on those machines. If I can’t, have a before and after sample and run them multiple times to get consistent results.

*Types* *of* *Profiling*

There are several types of profiling I can perform in Go.

*CPU* *profiling*

CPU profiling is the most common type of profile. When CPU profiling is enabled, the runtime will interrupt itself every ~10ms and record the stack trace of the currently running Goroutines. Once the profile is saved to disk, we can analyze it to determine the hottest code paths. The more times a function appears in the profile, the more time that code path is taking as a percentage of the total runtime.

*Memory* *profiling*

Memory profiling records the stack trace when a heap allocation is made. Memory profiling, like CPU profiling, is sample based. By default, samples are profiled at 1 alloc for every 512kb. This rate can be changed. Stack allocations are assumed to be free and are not tracked in the memory profile. Because memory profiling is sample based and because it tracks allocations not used, using memory profiling to determine my application's overall memory usage is difficult.

*Blocking* *profiling*

Blocking profiling is quite unique. A block profile is similar to a CPU profile, but it records the amount of time a Goroutine spent waiting for a shared resource. This can be useful for determining concurrency bottlenecks in my application. Blocking profiling can show me when a large number of Goroutines could make progress, but were blocked.

Blocking includes:
- Sending or receiving on an unbuffered channel.
- Sending to a full channel, receiving from an empty one.
- Trying to Lock a sync.Mutex that is locked by another Goroutine.
- Block profiling is a very specialized tool, it should not be used until I believe I’ve  eliminated all my CPU and memory usage bottlenecks.

*One* *profile* *at* *a* *time*

Profiling is not free. Profiling has a moderate, but measurable impact on program performance—especially if I increase the memory profile sample rate. Most tools will not stop me from enabling multiple profiles at once. If I enable multiple profiles at the same time, they will observe their own interactions and skew my results.

Do not enable more than one kind of profile at a time.

*Hints* *to* *interpret* *what* *I* *see* *in* *the* *profile*

If I see lots of time spent in runtime.mallocgc function, the program potentially makes an excessive amount of small memory allocations. The profile will tell me where the allocations are coming from. See the memory profiler section for suggestions on how to optimize this case.

If lots of time is spent in channel operations, sync.Mutex code and other synchronization primitives or system components, the program probably suffers from contention. Consider restructuring the program to eliminate frequently accessed shared resources. Common techniques for this include sharding/partitioning, local buffering/batching and copy-on-write technique.
If lots of time is spent in syscall.Read/Write, the program potentially makes an excessive amount of small reads and writes. Bufio wrappers around os.File or net.Conn can help in this case.

If lots of time is spent in the GC component, the program either allocates too many transient objects or the heap size is very small so garbage collections happen too frequently.

- Large objects affect memory consumption and GC pacing, while large numbers of tiny allocations affect marking speed.
- Combine values into larger values. This will reduce the number of memory allocations (faster) and also reduce pressure on the garbage collector (faster garbage collections).
- Values that do not contain any pointers are not scanned by the garbage collector. Removing pointers from actively used values can positively impact garbage collection time.

*Rules* *of* *Performance*

Basic rules around performance.

- 1. Never guess about performance.
- 2. Measurements must be relevant.
- 3. Profile before I decide something is performance critical.
- 4. Test to know I’m correct.

*Go* *and* *OS* *Tooling*

*time* *program*

The time command provides information that can help me get a sense how my program is performing.

*Perf* *program*

If I’m on linux, then perf(1) is a great tool for profiling applications. Since Go has frame pointers, perf can profile Go applications.

	$ go build -toolexec="perf stat" cmd/compile/internal/gc
	# cmd/compile/internal/gc
      Performance counter stats for '/home/dfc/go/pkg/tool/linux_amd64/compile -o $WORK/cmd/compile/internal/gc.a -trimpath $WORK -p cmd/compile/internal/gc -complete -buildid 87cd803267511b4d9e753d68b5b66a70e2f878c4 -D _/home/dfc/go/src/cmd/compile/internal/gc -I $WORK -pack ./alg.go ./align.go ./bexport.go ./bimport.go ./builtin.go ./bv.go ./cgen.go ./closure.go ./const.go ./cplx.go ./dcl.go ./esc.go ./export.go ./fmt.go ./gen.go ./go.go ./gsubr.go ./init.go ./inl.go ./lex.go ./magic.go ./main.go ./mpfloat.go ./mpint.go ./obj.go ./opnames.go ./order.go ./parser.go ./pgen.go ./plive.go ./popt.go ./racewalk.go ./range.go ./reflect.go ./reg.go ./select.go ./sinit.go ./sparselocatephifunctions.go ./ssa.go ./subr.go ./swt.go ./syntax.go ./type.go ./typecheck.go ./universe.go ./unsafe.go ./util.go ./walk.go':

       7026.140760 task-clock (msec)         #    1.283 CPUs utilized          
             1,665 context-switches          #    0.237 K/sec                  
                39 cpu-migrations            #    0.006 K/sec                  
            77,362 page-faults               #    0.011 M/sec                  
      21,769,537,949 cycles                  #    3.098 GHz                     [83.41%]
      11,671,235,864 stalled-cycles-frontend #   53.61% frontend cycles idle    [83.31%]
     6,839,727,058 stalled-cycles-backend    #   31.42% backend  cycles idle    [66.65%]
    27,157,950,447 instructions              #    1.25  insns per cycle        
                                             #    0.43  stalled cycles per insn [83.25%]
     5,351,057,260 branches                  #  761.593 M/sec                   [83.49%]
       118,150,150 branch-misses             #    2.21% of all branches         [83.15%]

       5.476816754 seconds time elapsed
      
** Example Code

I have a program that takes a stream of data looking for the name elvis with a lowercase e. If that name is found in the stream, the name is corrected by replacing the lowercase e with a capital E.

Listing 1

      var data = []struct {
            input  []byte
            output []byte
      }{
            {[]byte("abc"), []byte("abc")},
            {[]byte("elvis"), []byte("Elvis")},
            {[]byte("aElvis"), []byte("aElvis")},
            {[]byte("abcelvis"), []byte("abcElvis")},
            {[]byte("eelvis"), []byte("eElvis")},
            {[]byte("aelvis"), []byte("aElvis")},
            {[]byte("aabeeeelvis"), []byte("aabeeeElvis")},
            {[]byte("e l v i s"), []byte("e l v i s")},
            {[]byte("aa bb e l v i saa"), []byte("aa bb e l v i saa")},
            {[]byte(" elvi s"), []byte(" elvi s")},
            {[]byte("elvielvis"), []byte("elviElvis")},
            {[]byte("elvielvielviselvi1"), []byte("elvielviElviselvi1")},
            {[]byte("elvielviselvis"), []byte("elviElvisElvis")},
      }

This is a data table that represents the potential data from the stream. It lays out the input stream and the expected output stream.

Listing 2

      func assembleInputStream() []byte {
            var in []byte
            for _, d := range data {
                  in = append(in, d.input...)
            }
            return in
      }

      func assembleOutputStream() []byte {
            var out []byte
            for _, d := range data {
                  out = append(out, d.output...)
            }
            return out
      }

These functions assemble the input and output into a single stream for processing. 
With this in place, here is an algorithm I wrote to solve the problem.


Listing 3

      func algOne(data []byte, find []byte, repl []byte, output *bytes.Buffer) {
            input := bytes.NewBuffer(data)
            size := len(find)
            buf := make([]byte, size)
            end := size - 1

            if n, err := io.ReadFull(input, buf[:end]); err != nil {
                  output.Write(buf[:n])
                  return
            }

            for {
                  if _, err := io.ReadFull(input, buf[end:]); err != nil {
                        output.Write(buf[:end])
                        return
                  }

                  if bytes.Equal(buf, find) {
                        output.Write(repl)
                        if n, err := io.ReadFull(input, buf[:end]); err != nil {
                              output.Write(buf[:n])
                              return
                        }
                        continue
                  }

                  output.WriteByte(buf[0])
                  copy(buf, buf[1:])
            }
      }

My algorithm is based on the idea of creating a buffer of 5 bytes and comparing those 5 bytes with the lowercase version of elvis. If there is a match, then the uppercase version of Elvis is sent through the output stream. If there is no match, the first byte of the buffer is sliced off and a new byte from the input stream is added to the end of the buffer. Then those 5 bytes are compared again.

Luckily, my friend Tyler took the time to provide a different implementation that solves the same problem.


Listing 4

      func algTwo(data []byte, find []byte, repl []byte, output *bytes.Buffer) {
            input := bytes.NewReader(data)
            size := len(find)
            idx := 0

            for {
                  b, err := input.ReadByte()
                  if err != nil {
                        break
                  }

                  if b == find[idx] {
                        idx++
                        if idx == size {
                        output.Write(repl)
                        idx = 0
                        }
                        continue
                  }

                  if idx != 0 {
                        output.Write(find[:idx])
                        input.UnreadByte()
                        idx = 0
                        continue
                  }

                  output.WriteByte(b)
                  idx = 0
            }
      }

Tyler’s algorithm is based on the idea of reading one byte at a time out of the input stream and comparing that byte with the lowercase version of elvis based on a moving index. The algorithm starts with the index at zero, which means the first byte out of the input stream is compared with the lowercase e in elvis. If there is a match, the index is incremented so the next byte pulled from the input stream can be compared to the letter l, and so on, and so on.

If there are five matches in a row, the lowercase version of elvis is found and the uppercase version of Elvis is sent through the output stream. If there is no match, the index is reset to zero, the last byte is unread, the process starts over again. I found this solution by Tyler really cool.

** Benchmarking
With the two algorithms in place that solve the same exact problem in two different ways, the next step is to write benchmark functions to compare the two algorithms.


Listing 5

      var output bytes.Buffer
      var in = assembleInputStream()
      var find = []byte("elvis")
      var repl = []byte("Elvis")

      func BenchmarkAlgorithmOne(b *testing.B) {
            for i := 0; i < b.N; i++ {
                  output.Reset()
                  algOne(in, find, repl, &output)
            }
      }

      func BenchmarkAlgorithmTwo(b *testing.B) {
            for i := 0; i < b.N; i++ {
                  output.Reset()
                  algTwo(in, find, repl, &output)
            }
      }

With the benchmark functions in place, I can now compare the two algorithms using the testing tool. I will add the -benchmem flag so the tooling reports allocations.

Listing 6

      $ go test -bench . -benchtime 3s -benchmem

      goos: darwin
      goarch: amd64
      pkg: github.com/ardanlabs/gotraining/topics/go/profiling/memcpu
      cpu: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz

      BenchmarkAlgorithmOne-16   2120113    1594 ns/op   53 B/op   2 allocs/op
      BenchmarkAlgorithmTwo-16   9103246   387.7 ns/op    0 B/op   0 allocs/op

      PASS
      ok

I see two things in the results. First, Tyler’s algorithm is roughly 4 times faster than mine. Second, Tyler wrote a zero allocation algorithm and mine is allocating two values on the heap worth a total of 52 bytes.

At this point, I want to find and remove those two values allocating on the heap with the hope that maybe my algorithm will be as fast or at least closer to the same performance as Tyler’s algorithm. To find these allocations, I need a memory profile.

** Memory Profiling
To generate a memory profile I need to add the -memprofile flag to the go test call.


Listing 7

      $ go test -bench . -benchtime 3s -benchmem -memprofile p.out

      goos: darwin
      goarch: amd64
      pkg: github.com/ardanlabs/gotraining/topics/go/profiling/memcpu
      cpu: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz

      BenchmarkAlgorithmOne-16   2260207    1566 ns/op   53 B/op   2 allocs/op
      BenchmarkAlgorithmTwo-16   8515495   422.5 ns/op    0 B/op   0 allocs/op

      PASS
      ok

The output doesn’t change, but I do end up with two new files in the local directory.

Listing 8

      $ ls -la

      drwxr-xr-x   7 bill  staff   224B May  7 10:59 .
      drwxr-xr-x  14 bill  staff   448B Jan  4 08:16 ..
      -rw-r--r--   1 bill  staff   2.3K Apr 28 09:09 README.md
      -rwxr-xr-x   1 bill  staff   3.0M May  7 10:59 memcpu.test
      -rw-r--r--   1 bill  staff   488B May  7 10:59 p.out
      -rw-r--r--   1 bill  staff   4.6K Apr 30 15:56 stream.go
      -rw-r--r--   1 bill  staff   773B Apr 28 09:09 stream_test.go

I can see a p.out file that is fairly small at 488 bytes. This is the memory profile data I requested with the -memprofile flag. There is also a file named memcpu.test. This is the test binary that was built by the compiler to run the benchmarks. When I ask for a memory profile, the test binary is not removed after the benchmark is run. This is so extra information can be provided by the profile tooling.

Go has a profiling tool that I can use to review the profile data.

Listing 9

      $ go tool pprof memcpu.test p.out

      File: memcpu.test
      Type: alloc_space
      Time: May 7, 2021 at 10:59am (CDT)

      Entering interactive mode (type "help" for commands, "o" for options)
      (pprof)

I’m using the terminal based profile tooling, however if I want to use the browser version of the tooling, I just need to add the -http flag with a port number.

Listing 10

      $ go tool pprof -http :8080 memcpu.test p.out

For now I will stay in the terminal window. Since I know which function is allocating, I can ask the tool to provide information specific about the algOne function using the list command.

Listing 11

      (pprof) list algOne

      Total: 167.51MB
      12.50MB   167.51MB (flat, cum)   100% of Total
            .          .     78:
            .          .     79:// algOne is one way to solve the problem.
            .          .     80:func algOne(data []byte, find []byte, repl
            .          .     81:
            .          .     82:	// Use a bytes Buffer to provide a stream to
            .   155.01MB     83:	input := bytes.NewBuffer(data)
            .          .     84:
            .          .     85:	// The number of bytes we are looking for.
            .          .     86:	size := len(find)
            .          .     87:
            .          .     88:	// Declare the buffers we need to process the
      12.50MB    12.50MB     89:	buf := make([]byte, size)
            .          .     90:	end := size - 1
            .          .     91:
            .          .     92:	// Read in an initial number of bytes we need
            .          .     93:	if n, err := io.ReadFull(input, buf[:end]);
            .          .     94:		output.Write(buf[:n])
      (pprof)

The list command gives me a detailed view of a function and takes a regular expression to find the functions I want to see. Thanks to the list command, I can see there is an allocation happening on line 83 and on line 89.

Listing 12

      12.50MB |  167.51MB | (flat, cum)   100% of Total
      -----------|-----------|----------------------------------------------------
            . | 155.01MB  |  83:	input := bytes.NewBuffer(data)
      12.50MB |  12.50MB  |  89:	buf := make([]byte, size)
      (pprof)

At closer inspection I see there are two columns in the report. The first column represents flat allocations and the second represents cumulative allocations. Flat means the value allocated to the heap is represented on that line of code. Cumulative means the values allocating to the heap are represented inside the call chain originating from that function call.

I was looking for two allocations and it seems lines 83 and 89 contain the construction points for the two values allocating in the heap. That’s only part of the story. I still don’t know why and the profile can never tell me this.

Before I begin to try and understand why the values allocated, I want to see this same list view in the browser. I can do this by using the weblist command.


Listing 13

      (pprof) weblist algOne

The weblist command is similar to the list command except it brings up the browser tooling.

Figure 1

.image /tour/static/img/pro1.png


This is the browser based tool showing the same list output. One reason to use the browser over the terminal is when I include the test binary. This allows me to see the list output down to the assembly level.

Figure 2

.image /tour/static/img/pro2.png

When I click on the code at line 89, the assembly behind that line of code is shown. This only works when I provide the test binary.

If I look closer at the list output from the browser, I notice a difference from the terminal view. That is, the allocation on line 83 is a flat allocation.


Figure 3

.image /tour/static/img/pro3.png

See how the first column shows the same allocation number as the second column. This begs the question, is the allocation flat which means the input variable is being constructed on the heap, or is the allocation cumulative which means the allocation is coming from inside the bytes.NewBuffer function?

From my point of view, the browser view is more accurate because it takes into account an important compiler optimization that took place when the code ran called inlining.

** Inlining
Inlining is an optimization that removes function calls and replaces them with a copy of the code inside the function in question. For some reason the terminal view has inlining turned off.

How can I get the terminal to show the list command with inlining turned on?

Listing 14

      $ go tool pprof --noinlines  memcpu.test p.out

I need to add the --noinlines flag to the call to go tool pprof, which is supposed to turn off inlining, but for the terminal view it turns it on.


Listing 15

      (pprof) list algOne

      Total: 167.51MB
      167.51MB   167.51MB (flat, cum)   100% of Total
            .          .     78:
            .          .     79:// algOne is one way to solve the problem.
            .          .     80:func algOne(data []byte, find []byte, repl
            .          .     81:
            .          .     82:	// Use a bytes Buffer to provide a stream to
      155.01MB   155.01MB    83:	input := bytes.NewBuffer(data)
            .          .     84:
            .          .     85:	// The number of bytes we are looking for.
            .          .     86:	size := len(find)
            .          .     87:
            .          .     88:	// Declare the buffers we need to process the
      12.50MB    12.50MB     89:	buf := make([]byte, size)
            .          .     90:	end := size - 1
            .          .     91:
            .          .     92:	// Read in an initial number of bytes we need
            .          .     93:	if n, err := io.ReadFull(input, buf[:end]);
            .          .     94:		output.Write(buf[:n])
      (pprof)

Now the terminal view is showing line 83 as a flat allocation. How does inlining actually work? Start with the bytes.NewBuffer function which we know is being inlined.

Listing 16

      func NewBuffer(buf []byte) *Buffer {
            return &Buffer{buf: buf}
      }

I can see this is a factory function using pointer semantics. The caller gets shared access to the Buffer value being constructed. If the function is called, then the Buffer would have to be constructed in the heap. This is because of the ownership rule.

Any function that constructs a value, is the owner of that value. If the value needs to exist after the owning function returns, then the value must be constructed in the heap.

If the function can be inlined, the ownership of the construction moves up to the calling function. This means the algOne function owns the construction of Buffer.


Listing 17

      func algOne() {

            // Before inlining
            input := bytes.NewBuffer(data)       <-- Original Call

            // After inlining
            input := &bytes.Buffer{buf: data}    <-- After Inlining Optimization
      }

I can see that after the inlining optimization, the algOne function becomes the owner of the construction of the Buffer value, not the NewBuffer function. Therefore, the value doesn’t need to escape to the heap unless the value still needs to exist after the algOne function returns.

It’s important to note that Tyler’s algorithm is a zero allocation algorithm in part thanks to the same inlining optimization.

Listing 18

      func algOne() {

            // Before inlining
            input := bytes.NewReader(data)       <-- Original Call

            // After inlining
            input := &bytes.Reader{buf: data}    <-- After Inlining Optimization
      }

Since the NewReader function is inlined, the construction of the Reader value is owned by Tyler’s algorithm. Somehow Tyler is not doing anything in algTwo that is causing the Reader value to escape, where I am doing something in algOne with the Buffer value.

** Escape Analysis
How do I know inlining is absolutely happening and I still don’t know why algOne is causing allocations where algTwo is zero allocation? Only one tool can tell me the why and that’s the compiler. The compiler is making the decision using the escape analysis algorithm.

Luckily, the compiler builds the test binary before anything runs so I can ask the compiler to generate an escape analysis report before running the benchmark.

Listing 19

      $ go test -bench . -benchtime 3s -benchmem -memprofile p.out -gcflags -m=2

I added -gcflags -m=2 as flags on the call to go test. This produces an escape analysis report before running the benchmark.

Listing 20

      ./stream.go:83:26: inlining call to bytes.NewBuffer func([]byte) *bytes.Buffer { return &bytes.Buffer{...} }

      ./stream.go:83:26: &bytes.Buffer{...} escapes to heap:
      ./stream.go:83:26:   flow: ~R0 = &{storage for &bytes.Buffer{...}}:
      ./stream.go:83:26:     from &bytes.Buffer{...} (spill) at ./stream.go:83:26
      ./stream.go:83:26:     from ~R0 = <N> (assign-pair) at ./stream.go:83:26
      ./stream.go:83:26:   flow: input = ~R0:
      ./stream.go:83:26:     from input := (*bytes.Buffer)(~R0) (assign) at ./stream.go:83:8
      ./stream.go:83:26:   flow: io.r = input:
      ./stream.go:83:26:     from input (interface-converted) at ./stream.go:113:28
      ./stream.go:83:26:     from io.r, io.buf := input, buf[:end] (assign-pair) at ./stream.go:113:28
      ./stream.go:83:26:   flow: {heap} = io.r:
      ./stream.go:83:26:     from io.ReadAtLeast(io.r, io.buf, len(io.buf)) (call parameter) at ./stream.go:113:28

These are the lines in the escape analysis report related to line 83 from the list output. The very first line states the compiler is choosing to inline the call to bytes.NewBuffer. This is the proof I wanted to be absolutely sure the bytes.NewBuffer function was being inlined.

How does the compiler decide what functions can be inlined or not?

Listing 21

      ./stream.go:80:6: cannot inline algOne: function too complex: cost 636 exceeds budget 80

      ./stream.go:131:6: cannot inline algTwo: function too complex: cost 315 exceeds budget 80

Here are two lines from the report that show the scoring for the algOne and algTwo functions. I can see both functions scored above 80 points and therefore are not a candidate for inlining. The compiler uses a complex scoring system to determine if a function can be inlined and every function is scored.

The code for the inlining algorithm can be found in the inl.go file that is part of the compiler.

.play profiling/inl.go

How can I almost guarantee a factory function like NewBuffer or NewReader will score under 80? I need to make it a leaf function. In other words, make sure no other function calls are made so the call tree ends with the factory function.
Now I absolutely know the algOne function owns the construction of the bytes.Buffer value. I still don’t know why the input variable has allocated, especially since it doesn’t need to exist after the algOne function returns.

I need to look at the escape analysis report for more information.

Listing 22

      ./stream.go:83:26: &bytes.Buffer{...} escapes to heap:
      ./stream.go:83:26:   flow: ~R0 = &{storage for &bytes.Buffer{...}}:
      ./stream.go:83:26:     from &bytes.Buffer{...} (spill) at ./stream.go:83:26
      ./stream.go:83:26:     from ~R0 = <N> (assign-pair) at ./stream.go:83:26
      ./stream.go:83:26:   flow: input = ~R0:
      ./stream.go:83:26:     from input := (*bytes.Buffer)(~R0) (assign) at ./stream.go:83:8
      ./stream.go:83:26:   flow: io.r = input:
      ./stream.go:83:26:     from input (interface-converted) at ./stream.go:113:28
      ./stream.go:83:26:     from io.r, io.buf := input, buf[:end] (assign-pair) at ./stream.go:113:28
      ./stream.go:83:26:   flow: {heap} = io.r:
      ./stream.go:83:26:     from io.ReadAtLeast(io.r, io.buf, len(io.buf)) (call parameter) at ./stream.go:113:28

This section of the report has the final why. It’s sharing information about the construction of the bytes.Buffer on line 83 where the function was inlined.

Listing 23

      flow: io.r = input:
      from input (interface-converted) at ./stream.go:113:28
      from io.r, io.buf := input, buf[:end] (assign-pair) at ./stream.go:113:28

An interface conversion at line 113 seems to be the reason. What code is on line 113?

Listing 24

      113 if n, err := io.ReadFull(input, buf[:end]); err != nil {
      114    output.Write(buf[:n])
      115    return
      116 }

It’s the call to io.ReadFull and I can see the input variable is being passed into the function as the first parameter. Passing a value down the call stack doesn’t create an allocation so what’s different about this call? I need to look at the io.ReadFull function.

Listing 25

      func ReadFull(r Reader, buf []byte) (n int, err error) {
            return ReadAtLeast(r, buf, len(buf))
      }

I can see the io.ReadFull function is a polymorphic function. It’s accepting the input value being passed not based on what it is, but on what it does. It’s using the io.Reader interface to accept the data. That’s the reason behind the "interface-converted" and "assign-pair" message in the report. 

Tyler is not using the io package but the method set of the input variable, and he’s getting the inline optimization as well. This explains why Tyler doesn’t have an allocation. So if I want to get rid of this allocation on line 83, I need to stop using the io package and switch to the method set.

Listing 26

      func algOne(data []byte, find []byte, repl []byte, output *bytes.Buffer) {
            input := bytes.NewBuffer(data)
            size := len(find)
            buf := make([]byte, size)
            end := size - 1

            if n, err := input.Read(buf[:end]); err != nil {           <-- REPLACED
                  output.Write(buf[:n])
                  return
            }

            for {
                  var err error
                  buf[end:][0], err = input.ReadByte()                   <-- REPLACED
                  if err != nil {
                        output.Write(buf[:end])
                        return
                  }

                  if bytes.Equal(buf, find) {
                        output.Write(repl)
                        if n, err := input.Read(buf[:end]); err != nil {   <-- REPLACED
                        output.Write(buf[:n])
                        return
                        }
                        continue
                  }

                  output.WriteByte(buf[0])
                  copy(buf, buf[1:])
            }
      }

I have replaced the three calls using the io package for methods against the input variable. This should remove the allocation on line 83.


Listing 27

      $ go test -bench . -benchtime 3s -benchmem -memprofile p.out -gcflags -m=2

      goos: darwin
      goarch: amd64
      pkg: github.com/ardanlabs/gotraining/topics/go/profiling/memcpu
      cpu: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz

      BenchmarkAlgorithmOne-16    3658340	  975.3 ns/op    5 B/op   1 allocs/op
      BenchmarkAlgorithmTwo-16    9730435	  377.6 ns/op    0 B/op   0 allocs/op

      PASS
      ok

Sure enough I am down to 1 allocation worth 5 bytes. Where is this last 5 bytes allocating from? Time to use the list command again to see the line number and then inspect the escape analysis report.

Listing 28

      $ go tool pprof -noinlines p.out

      (pprof) list algOne
            20MB       20MB (flat, cum)   100% of Total
            .          .     83:
            .          .     84:	// The number of bytes we are looking for.
            .          .     85:	size := len(find)
            .          .     86:
            .          .     87:	// Declare the buffers we need to process the
            20MB       20MB     88:	buf := make([]byte, size)
            .          .     89:	end := size - 1
            .          .     90:
            .          .     91:	// Read in an initial number of bytes we need
            .          .     92:	if n, err := input.Read(buf[:end]); err != nil 
            .          .     93:		output.Write(buf[:n])

The final allocation is on line 88 related to the construction of the slice value.

Listing 29

      ./stream.go:88:13: make([]byte, size) escapes to heap:
      ./stream.go:88:13:   flow: {heap} = &{storage for make([]byte, size)}:
      ./stream.go:88:13:     from make([]byte, size) (non-constant size) at ./stream.go:88:13

The report is stating "non-constant size" which means the compiler doesn’t know the size of the backing array at compile time. This can be fixed if I hard code the value of 5 on the call to make. If I only ever search for a 5 byte string, then I’ll have no problems with this change. Hahahaha!


Listing 30

      func algOne(data []byte, find []byte, repl []byte, output *bytes.Buffer) {
            input := bytes.NewBuffer(data)
            size := len(find)
            buf := make([]byte, 5)    <-- REPLACED

            . . .
      }

After changing out the size variable for the literal number 5 on the call to make, does the allocation go away?

Listing 31

      $ go test -bench . -benchtime 3s -benchmem -memprofile p.out -gcflags -m=2

      goos: darwin
      goarch: amd64
      pkg: github.com/ardanlabs/gotraining/topics/go/profiling/memcpu
      cpu: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz

      BenchmarkAlgorithmOne-16    4129378	   868.5 ns/op    0 B/op   0 allocs/op
      BenchmarkAlgorithmTwo-16    9716834	   376.0 ns/op    0 B/op   0 allocs/op

      PASS
      ok

Now I have a zero allocation algorithm. However, Tyler’s algorithm is still faster than mine. The next thing I can do is run a cpu profile to possibly find some inefficient code that can be changed.

Listing 32

      $ go test -bench . -benchtime 3s -benchmem -cpuprofile p.out

      goos: darwin
      goarch: amd64
      pkg: github.com/ardanlabs/gotraining/topics/go/profiling/memcpu
      cpu: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz

      BenchmarkAlgorithmOne-16    4079672	   906.4 ns/op    0 B/op   0 allocs/op
      BenchmarkAlgorithmTwo-16    8689845	   414.3 ns/op    0 B/op   0 allocs/op

      PASS
      ok

By using the -cpuprofile flag instead of the -memprofile flag, I will get a cpu profile.


Listing 33

      $ go tool pprof p.out

      Type: cpu
      (pprof) list algOne

            950ms      3.97s (flat, cum) 53.36% of Total
                  .          .     87:	// Declare the buffers we need to process the
                  .          .     88:	buf := make([]byte, 5)
                  .          .     89:	end := size - 1
                  .          .     90:
                  .          .     91:	// Read in an initial number of bytes we need
              110ms      120ms     92:	if n, err := input.Read(buf[:end]); err != nil 
                  .          .     93:		output.Write(buf[:n])
                  .          .     94:		return
                  .          .     95:	}
                  .          .     96:
                  .          .     97:	for {
                  .          .     98:
                  .          .     99:		// Read in one byte from the input
                  .          .    100:		var err error
              310ms      440ms    101:		buf[end:][0], err = input.ReadByte()
                  .          .    102:		if err != nil {
                  .          .    103:
                  .          .    104:			// Flush the reset of the bytes
                  .          .    105:			output.Write(buf[:end])
                  .          .    106:			return
                  .          .    107:		}
                  .          .    108:
                  .          .    109:		// If we have a match, replace the
                  .      1.70s    110:		if bytes.Equal(buf, find) {
              270ms      650ms    111:			output.Write(repl)
                  .          .    112:
                  .          .    113:			// Read a new initial number of
                  .      240ms    114:			if n, err := input.Read(buf[:end])
                  .       10ms    115:				output.Write(buf[:n])
                  .          .    116:				return
                  .          .    117:			}
                  .          .    118:
                  .          .    119:			continue
                  .          .    120:		}
                  .          .    121:
                  .          .    122:		// Write the front byte since it has
              100ms      650ms    123:		output.WriteByte(buf[0])
                  .          .    124:
                  .          .    125:		// Slice that front byte out.
              160ms      160ms    126:		copy(buf, buf[1:])
                  .          .    127:	}
                  .          .    128:}
                  .          .    129:

Looks like there is almost four seconds of cumulative time taken in the algOne function and the majority of it is on line 110.

Listing 34

            .          .    109:		// If we have a match, replace the
            .      1.70s    110:		if bytes.Equal(buf, find) {
        270ms      650ms    111:			output.Write(repl)


So if I could find a way to replace the call to bytes.Equal, I could possibly get even closer to the same performance as Tyler’s algorithm.

Here is the reality. I should have just gone with Tyler’s algorithm from the beginning if that level of performance was really needed. Honestly, my algorithm really wasn’t that slow using the io package Read functions. Even after removing the allocations, I’m only looking at a difference of ~500 nanoseconds per operation.

** Profiling Live Code
In this section, I will learn how to profile code that is running. This will require the use of the http package in the standard library. It’s best I read chapter 10 before reading this chapter. That chapter goes deep into the use of the profile tooling and that knowledge is assumed here.

** Example Code
I have a program that implements a search engine using news feeds as the searchable content. The news feeds are downloaded and cached when the program starts.

This is the code that is being used in this section.
[[https://github.com/ardanlabs/gotraining/tree/master/topics/go/profiling/project][https://github.com/ardanlabs/gotraining/tree/master/topics/go/profiling/project]]

To start, I need to build and run the project.

Listing 35

      $ cd <Cloned Location>/topics/go/profiling/project
      $ go build
      $ ./project

      2021/05/10 11:29:21.616793 service.go:64: Listening on: 0.0.0.0:5000

Once the project is running, I can open the browser to run the website by using the URL "localhost:5000/search".


Figure 4

.image /tour/static/img/pro4.png

[[http://localhost:5000/search][http://localhost:5000/search]]


This is what the logs look like after running a search for the term "biden".

If I look at the logs, I can see 12 feeds were downloaded and cached from the BBC, CNN, and the NY Times. 

Listing 36

      service.go:64: Listening on: 0.0.0.0:5000
      rss.go:109: reloaded cache http://feeds.bbci.co.uk/news/rss.xml
      rss.go:109: reloaded cache http://feeds.bbci.co.uk/news/world/rss.xml
      rss.go:109: reloaded cache http://rss.cnn.com/rss/cnn_topstories.rss
      rss.go:109: reloaded cache http://feeds.bbci.co.uk/news/politics/rss.xml
      rss.go:109: reloaded cache http://rss.cnn.com/rss/cnn_world.rss
      rss.go:109: reloaded cache http://feeds.bbci.co.uk/news/world/us_and_canada/rss.xml
      rss.go:109: reloaded cache http://rss.cnn.com/rss/cnn_us.rss
      rss.go:109: reloaded cache http://rss.cnn.com/rss/cnn_allpolitics.rss
      rss.go:109: reloaded cache http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml
      rss.go:109: reloaded cache http://rss.nytimes.com/services/xml/rss/nyt/US.xml
      rss.go:109: reloaded cache http://rss.nytimes.com/services/xml/rss/nyt/Politics.xml
      rss.go:109: reloaded cache http://rss.nytimes.com/services/xml/rss/nyt/Business.xml


One of the first checks that I like to perform is to generate a garbage collection (GC) trace for the program while it’s executing a fixed amount of load. This will provide an initial understanding if there is a memory leak and it’s nice to gather some stats about how much work the garbage collector is performing.

** Generating a GC Trace
Before I run any load through the program, I want to look at an initial GC trace that is produced from the first search. This is when the feeds are downloaded for the first time. To see a GC trace, I need to use the GODEBUG environment variable when running the project.

Listing 37

      $ GODEBUG=gctrace=1 ./project > /dev/null

The GODEBUG variable has several different options, but this is how I can ask the Go runtime to write a trace to stderr each time the GC runs. Since I don’t want my logs to compete with the trace output, I’m writing anything being sent to stdout to the /dev/null device.

If I restart the program using these options and hit the search button again, I will see 4 GC’s take place.

Listing 38

      gc 1 @95.466s 0%: 0.042+0.54+0.026 ms clock, 0.67+0.32/0.73/0.29+0.43 ms cpu, 4->4->0 MB, 5 MB goal, 16 P

      gc 2 @95.635s 0%: 0.031+0.39+0.029 ms clock, 0.51+0.24/0.80/0.68+0.47 ms cpu, 4->4->1 MB, 5 MB goal, 16 P

      gc 3 @95.707s 0%: 0.047+0.40+0.012 ms clock, 0.76+0.072/0.92/1.0+0.20 ms cpu, 4->4->1 MB, 5 MB goal, 16 P

      gc 4 @95.976s 0%: 0.048+0.47+0.016 ms clock, 0.78+0.13/1.0/1.3+0.25 ms cpu, 4->4->2 MB, 5 MB goal, 16 P

Seeing 4 GC’s makes sense since the GC will initially run when the first 4 meg of memory is allocated on the heap. Since the program is downloading and caching the feeds, the other 3 GC’s happen quickly as the GC tries to keep the heap at 4 meg.

Here is a breakdown of the 4th GC that took place.


Listing 39

      // General   : [gc 4 @95.976s 0%:]
      gc 4         : The 4th GC run since the program started
      @95.976s     : 95.976 seconds since the program started
      0%           : Zero percent of the programs time has been spent in GC

      // Wall Clock: [0.048+0.47+0.016 ms clock]
      0.048ms      : STW        : Mark Setup       - Write Barrier on
      0.47ms       : Concurrent : Marking
      0.016ms      : STW        : Mark Termination - Write Barrier off / Clean Up

      // CPU Clock : [0.78+0.13/1.0/1.3+0.25 ms cpu]
      0.78ms       : STW        : Mark Setup
      0.13ms       : Concurrent : Mark - Assist Time
      1.0ms        : Concurrent : Mark - Background GC time
      1.3ms        : Concurrent : Mark - Idle GC time
      0.25ms       : STW        : Mark Termination

      // Memory    : [4->4->2 MB]
      4MB          : Heap memory in-use before the Marking started
      4MB          : Heap memory in-use after the Marking finished
      2MB          : Heap memory marked as live after the Marking finished

      // Goal      : [5 MB goal]
      5MB          : Collection goal for heap memory in-use after Marking finished

      // Threads   : [16 P]
      16P          : Number of logical P’s or threads used to run Goroutines

The comments explain what each of the different numbers mean. For my purpose in evaluating potential memory leaks and if the GC is overworking, the memory numbers and the percent of time in GC is what I will evaluate.

** Generating Load And Evaluation
To apply load on the program, I will use a tool named hey. The tool can be found on Github at [[https://github.com/rakyll/hey][https://github.com/rakyll/hey]].

With the service already started in one terminal, I will run the hey tool in a second terminal to send 10k requests over 100 concurrent connections.

Listing 40

      $ hey -m POST -c 100 -n 10000 "http://localhost:5000/search?term=biden&cnn=on&bbc=on&nyt=on"

Once the load is sent into the program, I will see GC traces flowing. After the last request is processed, the hey tooling provides the results.


Listing 41

      Hey Summary:
      Total:	2.6945 secs
      Slowest:	0.2664 secs
      Fastest:	0.0011 secs
      Average:	0.0248 secs
      Requests/sec:	3711.3009
      ----------------------------------------------------------------------------
      GC Trace (last 4):
      gc 837 @181.810s 0%: 0.038+0.64+0.027 ms clock, 0.61+0.24/1.6/2.2+0.44 ms cpu, 6->7->3 MB, 8 MB goal, 16 P
      gc 838 @181.812s 0%: 0.030+0.60+0.052 ms clock, 0.48+0.38/1.3/2.4+0.84 ms cpu, 6->6->3 MB, 7 MB goal, 16 P
      gc 839 @181.815s 0%: 0.044+0.65+0.032 ms clock, 0.71+0.34/1.8/1.3+0.52 ms cpu, 6->7->3 MB, 7 MB goal, 16 P
      gc 840 @181.819s 0%: 0.037+0.56+0.035 ms clock, 0.59+0.14/1.4/2.6+0.56 ms cpu, 7->7->3 MB, 8 MB goal, 16 P

The hey report doesn’t seem terribly bad, and neither does the end of the GC trace. It seems that 836 GC’s needed to take place to complete the work. I’m subtracting the first 4 GC’s that took place before I ran the load. The program was able to process 3711 requests a second, with the slowest request taking 266ms.

I’m confident there is no memory leak since the memory is at 7 meg in use and 3 meg marked live. In fact, if I wait a bit, the GC will force itself to run and I can see if anything is reduced more.

Listing 42

      GC forced
      gc 841 @302.003s 0%: 0.072+0.84+0.005 ms clock, 1.1+0/2.4/4.5+0.083 ms cpu, 5->5->3 MB, 6 MB goal, 16 P

      GC forced
      gc 842 @422.004s 0%: 0.10+1.0+0.004 ms clock, 1.7+0/3.2/3.2+0.077 ms cpu, 3->3->2 MB, 6 MB goal, 16 P

      GC forced
      gc 843 @542.255s 0%: 0.096+1.0+0.004 ms clock, 1.5+0/3.3/4.8+0.071 ms cpu, 2->2->2 MB, 4 MB goal, 16 P

I can see after waiting a few minutes, the memory in use on the heap dropped to 2 meg, which matches what is live. This is a good sign of no memory leak.

** Adding Profile Endpoints
With these performance numbers from hey, what I need next is a memory profile that is representative of these 10k requests I sent through the program. Luckily, I can get this profile because I set up a couple things in the code.


Listing 43

      package main

      import (
      _ "net/http/pprof" // call init function
      )

The first thing to do is add this import to the source code file hosting the main function. This import allows the compiler to find an init function in the pprof package that sets debug routes in the default server mux.

Listing 44

      src/net/http/pprof/pprof.go

      func init() {
            http.HandleFunc("/debug/pprof/", Index)
            http.HandleFunc("/debug/pprof/cmdline", Cmdline)
            http.HandleFunc("/debug/pprof/profile", Profile)
            http.HandleFunc("/debug/pprof/symbol", Symbol)
            http.HandleFunc("/debug/pprof/trace", Trace)
      }

Next, I need to bind the default server mux to an ip address and port. I want this to be separate from the application traffic so these endpoints can be protected behind a firewall.

Listing 45

      package main

      import (
            _ "net/http/pprof" // call init function
      )

      func main() {
            debugHost := ":5000"
            
            . . .

            go func() {
                  log.Printf("main: Debug Listening %s", debugHost)
                  err := http.ListenAndServe(debugHost, http.DefaultServeMux)
                  if err != nil {
                  log.Printf("main: Debug Listener closed: %v", err)
                  }
            }()

            . . .
      }

A Goroutine is created to block on the http.ListenAndServe call to handle the debug routes. Since I already set this up before I ran load through the program, I can use the debug/pprof endpoint now to get a memory profile.


Figure 5

.image /tour/static/img/pro5.png

[[http://localhost:5000/debug/pprof][http://localhost:5000/debug/pprof]]


There is so much profiling data I can extract from my running program. In this case, I am interested in the heap profile, which is available from the very first link. If I click on the allocs link, I can see what a raw memory profile looks like.

Figure 6

.image /tour/static/img/pro6.png

[[http://localhost:5000/debug/pprof/allocs?debug=1][http://localhost:5000/debug/pprof]]


The raw memory profile breaks things down by stack, however at the very top there is some information that’s interesting.

Listing 46

      heap profile: 7: 59424 [12335: 286125512] @ heap/1048576

      [7]          Currently live objects,
      [59424]      Amount of memory occupied by live objects
      [12335]      Total number of allocations
      [286125512]  Amount of memory occupied by all allocations

      Those first four numbers represent program wide information for this snapshot of the memory profile. Then for each stack, there is similar information.

      Listing 11.13
      1: 40960 [245: 10035200] @

      [1]          Currently live objects,
      [40960]      Amount of memory occupied by live objects
      [245]        Total number of allocations
      [10035200]   Amount of memory occupied by all allocations

It’s nice to have some understanding of the raw numbers, but this is not a productive way to read the profile data.

** Viewing Memory Profile
I’m going to use the profile tooling as explained in chapter 10 to explore the memory profile. What’s cool is I can give the pprof tool the URL to the memory profile and it can use that to read it in.

Listing 47

      $ go tool pprof -noinlines http://localhost:5000/debug/pprof/allocs

      Fetching profile over HTTP from http://localhost:5000/debug/pprof/allocs
      Type: alloc_space
      Time: May 10, 2021 at 1:48pm (CDT)

      Entering interactive mode (type "help" for commands, "o" for options)
      (pprof)

The problem now is I don’t know what I’m specifically looking for. I really want to find any low hanging fruit, therefore I will use the top command to get a list of the top 15 functions that are allocating the most memory.


Listing 48

      (pprof) top 15 -cum

      Showing nodes accounting for 3300.39MB, 53.67% of 6148.89MB total
      Dropped 90 nodes (cum <= 30.74MB)
      Showing top 15 nodes out of 47
           flat  flat%   sum%        cum   cum%
              0     0%     0%  3675.30MB 59.77%  net/http.(*conn).serve
       385.68MB  6.27%  6.27%  3653.77MB 59.42%  github.com/ardanlabs/gotraining/topics/go/profiling/project/service.handler
              0     0%  6.27%  3653.77MB 59.42%  github.com/braintree/manners.(*gracefulHandler).ServeHTTP
              0     0%  6.27%  3653.77MB 59.42%  net/http.(*ServeMux).ServeHTTP
              0     0%  6.27%  3653.77MB 59.42%  net/http.HandlerFunc.ServeHTTP
              0     0%  6.27%  3653.77MB 59.42%  net/http.serverHandler.ServeHTTP
       617.76MB 10.05% 16.32%  3055.01MB 49.68%  github.com/ardanlabs/gotraining/topics/go/profiling/project/service.render
        48.01MB  0.78% 17.10%  2445.06MB 39.76%  github.com/ardanlabs/gotraining/topics/go/profiling/project/search.rssSearch
            1MB 0.016% 17.12%  2437.26MB 39.64%  github.com/ardanlabs/gotraining/topics/go/profiling/project/service.executeTemplate
              0     0% 17.12%  2436.26MB 39.62%  html/template.(*Template).Execute
         2.50MB 0.041% 17.16%  2436.26MB 39.62%  text/template.(*Template).execute
              0     0% 17.16%  2433.76MB 39.58%  text/template.(*state).walk
      2235.93MB 36.36% 53.52%  2393.53MB 38.93%  strings.ToLower
         9.51MB  0.15% 53.67%  2006.99MB 32.64%  github.com/ardanlabs/gotraining/topics/go/profiling/project/search.CNN.Search
              0     0% 53.67%  1935.06MB 31.47%  fmt.Fprint

      (pprof)

I added the `-cum` switch to the top command to sort the list by the cumulative value. Seeing the socket connection and mux at the top of the list makes sense since all traffic flows through that code. Having the template calls next makes sense because rendering HTML will produce allocations.

What comes next is the call to the search.rssSearch function.

Listing 49

            flat  flat%   sum%        cum   cum%
         48.01MB  0.78% 17.10%  2445.06MB 39.76%  github.com/ardanlabs/gotraining/topics/go/profiling/project/search.rssSearch

Top is telling me that this function represents 39.76% of the total allocations made which is worth a total of 2.4 gig. This is a function that I should look closer at. Maybe I can find some non-productive allocations that could be removed to reduce the amount of GC that needs to take place to complete this work. If I can reduce allocations, and therefore reduce the number of GC’s, I will get better performance.

I will use the list command to review the search.rssSearch function.

Listing 50

      (pprof) list rssSearch

      Total: 6GB
      ROUTINE ======================== project/search/rss.go
      48.01MB     2.39GB (flat, cum) 39.76% of Total
      20.50MB    20.50MB     79:	var d Document
            .     3.52MB    102:			if err := xml.NewDecoder(resp.Bod
            .     2.34GB    119:		if strings.Contains(strings.ToLower(ite
      27.51MB    27.51MB    120:			results = append(results, Result{

      (pprof)

I’ve trimmed down the list output to just those lines that are showing allocations. Out of the 2.39 GB of allocation coming from this function, line 119 represents 2.34GB of that. That’s the line of code I need to focus on.

Listing 51

      118     for _, item := range d.Channel.Items {
      119         if strings.Contains(strings.ToLower(item.Description), strings.ToLower(term)) {
      120             results = append(results, Result{
      121                 Engine:  engine,
      122                 Title:   item.Title,
      123                 Link:    item.Link,
      124                 Content: item.Description,
      125             })
      126         }
      127     }

When I look at the code on line 119 from the search/rss.go file, I see there is a call to strings.Contains and strings.ToLower inside a loop. That loop is checking if the search term matches against each item description for one specific feed. Since I have 12 feeds, for each request this loop is run 12 separate times.

I know a large number of allocations are happening here on line 119, but how can I tell if it’s because of the call to strings.Contains or strings.ToLower. Looking at a call graph that is isolated to the search.rssSearch function will help.

Listing 52

      (pprof) web rssSearch

For the call graph to be generated, it’s important to make sure Graphviz is installed. This is the website for more information: https://www.graphviz.org/

Figure 7

.image /tour/static/img/pro7.png


The call graph makes it clear that the call to strings.ToLower is causing all the allocations.

** Removing Allocations
Knowing this, I need to find a way to remove the call to strings.ToLower.

Listing 53

      118     for _, item := range d.Channel.Items {
      119         if strings.Contains(strings.ToLower(item.Description), strings.ToLower(term)) {
      120             results = append(results, Result{
      121                 Engine:  engine,
      122                 Title:   item.Title,
      123                 Link:    item.Link,
      124                 Content: item.Description,
      125             })
      126         }
      127     }

On closer inspection there are two things I can do.

Listing 54

      122     term = strings.ToLower(term)                        <-- ADDED
      123
      124     for _, item := range d.Channel.Items {
      125         if strings.Contains(item.Description, term) {   <-- CHANGED
      126             results = append(results, Result{
      127                 Engine:  engine,
      128                 Title:   item.Title,
      129                 Link:    item.Link,
      130                 Content: item.Description,
      131             })
      132         }
      133     }

First, I move the call to make the term lower outside of the loop. Having that call in the loop was really bad since it was performing the same operation over and over. That was producing a lot of little values on the heap. Second, I removed the call to make the description lower. That still needs to take place, so I moved it up with the code that caches the feeds.

Listing 55

       93     resp, err := http.Get(uri)
       94     if err != nil {
       95         return []Result{}, err
       96     }
       97     defer resp.Body.Close()
       98
       99     if err := xml.NewDecoder(resp.Body).Decode(&d); err != nil {
      100         return []Result{}, err
      101     }
      102
      103     for i := range d.Channel.Items {                        <-- ADDED
      104        lower := strings.ToLower(d.Channel.Items[i].Description)
      105        d.Channel.Items[i].Description = lower
      106     }
      107
      108     cache.Set(uri, d, expiration)

The new code converts the description to lowercase before storing the feed in the cache. This eliminates the need to do this inside the loop. Granted I should create a new field for the lowercase version of the description.

With these changes in place, I can build and run the program again. Then apply the same exact load and get the new results.


Listing 56

      Summary:
      Total:	1.4367 secs
      Slowest:	0.1706 secs
      Fastest:	0.0005 secs
      Average:	0.0131 secs
      Requests/sec:	6960.6340
      ----------------------------------------------------------------------------
      gc 484 @7.989s 4%: 0.038+0.76+0.025 ms clock, 0.62+0.52/1.5/1.8+0.40 ms cpu, 6->7->4 MB, 7 MB goal, 16 P
      gc 485 @7.991s 4%: 0.056+0.83+0.094 ms clock, 0.91+0.48/2.3/1.9+1.5 ms cpu, 7->8->4 MB, 9 MB goal, 16 P
      gc 486 @7.994s 4%: 0.089+0.65+0.023 ms clock, 1.4+0.55/1.8/1.2+0.38 ms cpu, 7->8->4 MB, 8 MB goal, 16 P
      gc 487 @7.996s 4%: 0.041+0.47+0.012 ms clock, 0.65+0.22/1.5/2.0+0.20 ms cpu, 7->8->4 MB, 8 MB goal, 16 P

With the code changes there are definite improvements. Now it seems that only 483 GC’s needed to take place to complete the work. I’m subtracting the first 4 GC’s again that took place before I ran the load. The program was able to process 6960 requests a second, with the slowest request taking 170ms. That’s an improvement of 88% on the requests per second.

If I wanted to focus more on a specific function that the top command might present, I can go back to chapter 10 and use benchmarks.



** The Basics of Profiling: Recap

"Those who can make you believe absurdities can make you commit atrocities" - Voltaire

How does a profiler work?

A profiler runs your program and configures the operating system to interrupt it at regular intervals. This is done by sending SIGPROF to the program being profiled, which suspends and transfers execution to the profiler. The profiler then grabs the program counter for each executing thread and restarts the program.

Profiling do's and don't's

Before you profile, you must have a stable environment to get repeatable results.

- The machine must be idle—don't profile on shared hardware, don't browse the web while waiting for a long benchmark to run.
- Watch out for power saving and thermal scaling.
- Avoid virtual machines and shared cloud hosting; they are too noisy for consistent measurements.

If you can afford it, buy dedicated performance test hardware. Rack them, disable all the power management and thermal scaling and never update the software on those machines.

For everyone else, have a before and after sample and run them multiple times to get consistent results.

** Types of Profiling

*CPU* *profiling* 

CPU profiling is the most common type of profile. When CPU profiling is enabled, the runtime will interrupt itself every 10ms and record the stack trace of the currently running goroutines. Once the profile is saved to disk, we can analyse it to determine the hottest code paths. The more times a function appears in the profile, the more time that code path is taking as a percentage of the total runtime.

*Memory* *profiling*  

Memory profiling records the stack trace when a heap allocation is made. Memory profiling, like CPU profiling is sample based. By default memory profiling samples 1 in every 1000 allocations. This rate can be changed. Stack allocations are assumed to be free and are not tracked in the memory profile. Because of memory profiling is sample based and because it tracks allocations not use, using memory profiling to determine your application's overall memory usage is difficult.

*Block* *profiling*  

Block profiling is quite unique. A block profile is similar to a CPU profile, but it records the amount of time a goroutine spent waiting for a shared resource. This can be useful for determining concurrency bottlenecks in your application. Block profiling can show you when a large number of goroutines could make progress, but were blocked. 

Blocking includes:

- Sending or receiving on an unbuffered channel.
- Sending to a full channel, receiving from an empty one.
- Trying to Lock a sync.Mutex that is locked by another goroutine.
- Block profiling is a very specialised tool, it should not be used until you believe you have eliminated all your CPU and memory usage bottlenecks.

*One* *profile* *at* *a* *time* 

Profiling is not free. Profiling has a moderate, but measurable impact on program performance—especially if you increase the memory profile sample rate. Most tools will not stop you from enabling multiple profiles at once. If you enable multiple profiles at the same time, they will observe their own interactions and skew your results.

Do not enable more than one kind of profile at a time

Hints to interpret what you see in the profile

If you see lots of time spent in `runtime.mallocgc` function, the program potentially makes excessive amount of small memory allocations. The profile will tell you where the allocations are coming from. See the memory profiler section for suggestions on how to optimize this case.

If lots of time is spent in channel operations, `sync.Mutex` code and other synchronization primitives or System component, the program probably suffers from contention. Consider to restructure program to eliminate frequently accessed shared resources. Common techniques for this include sharding/partitioning, local buffering/batching and copy-on-write technique.

If lots of time is spent in `syscall.Read/Write`, the program potentially makes excessive amount of small reads and writes. Bufio wrappers around os.File or net.Conn can help in this case.

If lots of time is spent in GC component, the program either allocates too many transient objects or heap size is very small so garbage collections happen too frequently.

- Large objects affect memory consumption and GC time, while large number of tiny allocations affects execution speed.

- Combine values into larger values. This will reduce number of memory allocations (faster) and also reduce pressure on garbage collector (faster garbage collections).

- Values that do not contain any pointers are not scanned by garbage collector. Removing pointers from actively used value can positively impact garbage collection time.

** Rules of Performance 

- 1) Never guess about performance.  
- 2) Measurements must be relevant.  
- 3) Profile before you decide something is performance critical.  
- 4) Test to know you are correct. 

** Installing Tools

*hey*  

hey is a modern HTTP benchmarking tool capable of generating the load you need to run tests. It's built using the Go language and leverages goroutines for behind the scenes async IO and concurrency.

	go get -u github.com/rakyll/hey


** Dave Cheney's Profiling Presentation:  

Much of what I have learned comes from Dave and working on solving problems. This slide deck is a great place to start. Much of this material can be found in the material below.

[[http://go-talks.appspot.com/github.com/davecheney/presentations/seven.slide#1][Seven ways to profile a Go program]]    

** Profiling, Debugging and Optimization Reading

Here is more reading and videos to also help get you started.

- [[https://github.com/davecheney/gophercon2018-performance-tuning-workshop/blob/master/1-welcome/introduction.md][The past and future of Microprocessor performance]] - Dave Cheney
- [[https://www.ardanlabs.com/blog/2017/05/language-mechanics-on-escape-analysis.html][Language Mechanics On Escape Analysis]] - William Kennedy  
- [[http://golang.org/blog/profiling-go-programs][Profiling Go Programs]] - Go Team  
- [[https://www.youtube.com/watch?v=xxDZuPEgbBU][Profiling & Optimizing in Go]] - Brad Fitzpatrick  
- [[https://www.youtube.com/watch?v=a9xrxRsIbSU][Go Dynamic Tools]] - Dmitry Vyukov  
- [[https://www.youtube.com/watch?v=lJ8ydIuPFeU&feature=youtu.be][How NOT to Measure Latency]] - Gil Tene  
- [[http://jmoiron.net/blog/go-performance-tales][Go Performance Tales]] - Jason Moiron  
- [[https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs][Debugging performance issues in Go programs]] - Dmitry Vyukov  
- [[https://methane.github.io/2015/02/reduce-allocation-in-go-code][Reduce allocation in Go code]] - Python Bytes  
- [[http://go-talks.appspot.com/github.com/davecheney/presentations/writing-high-performance-go.slide][Write High Performance Go]] - Dave Cheney  
- [[https://golang.org/lib/godoc/analysis/help.html][Static analysis features of godoc]] - Go Team   
- [[https://www.bigmarker.com/remote-meetup-go/Seven-ways-to-profile-a-Go-program][Seven ways to profile a Go program]] - Dave Cheney   
- [[https://github.com/golang/go/issues/16293][runtime: goroutine execution stalled during GC]] - Caleb Spare  
- [[http://www.thedotpost.com/2016/10/rhys-hiltner-go-execution-tracer][Go's execution tracer]] - Rhys Hiltner  
- [[https://rakyll.org/instruments][Using Instruments to profile Go programs]] - JBD    
- [[https://www.youtube.com/watch?v=nsM_m4hZ-bA&t=973s][Fighting latency: the CPU profiler is not your ally]] - Filippo Valsorda  
- [[https://making.pusher.com/go-tool-trace/][go tool trace]] - Will Sewell  

** Go and OS Tooling

*time*

The *time* command provide information that can help you get a sense how your program is performing.

Use the --time-- command to see data about building the program.
	$ cd $GOPATH/src/github.com/ardanlabs/gotraining/topics/go/profiling/project
	$ /usr/bin/time -lp go build		-- Mac OS X
	$ /usr/bin/time -v go build		-- Linux

*perf*

If you're a linux user, then perf(1) is a great tool for profiling applications. Now we have frame pointers, perf can profile Go applications.

	$ go build -toolexec="perf stat" cmd/compile/internal/gc
	# cmd/compile/internal/gc
      Performance counter stats for '/home/dfc/go/pkg/tool/linux_amd64/compile -o $WORK/cmd/compile/internal/gc.a -trimpath $WORK -p cmd/compile/internal/gc -complete -buildid 87cd803267511b4d9e753d68b5b66a70e2f878c4 -D _/home/dfc/go/src/cmd/compile/internal/gc -I $WORK -pack ./alg.go ./align.go ./bexport.go ./bimport.go ./builtin.go ./bv.go ./cgen.go ./closure.go ./const.go ./cplx.go ./dcl.go ./esc.go ./export.go ./fmt.go ./gen.go ./go.go ./gsubr.go ./init.go ./inl.go ./lex.go ./magic.go ./main.go ./mpfloat.go ./mpint.go ./obj.go ./opnames.go ./order.go ./parser.go ./pgen.go ./plive.go ./popt.go ./racewalk.go ./range.go ./reflect.go ./reg.go ./select.go ./sinit.go ./sparselocatephifunctions.go ./ssa.go ./subr.go ./swt.go ./syntax.go ./type.go ./typecheck.go ./universe.go ./unsafe.go ./util.go ./walk.go':

       7026.140760 task-clock (msec)         #    1.283 CPUs utilized          
             1,665 context-switches          #    0.237 K/sec                  
                39 cpu-migrations            #    0.006 K/sec                  
            77,362 page-faults               #    0.011 M/sec                  
      21,769,537,949 cycles                  #    3.098 GHz                     [83.41%]
      11,671,235,864 stalled-cycles-frontend #   53.61% frontend cycles idle    [83.31%]
     6,839,727,058 stalled-cycles-backend    #   31.42% backend  cycles idle    [66.65%]
    27,157,950,447 instructions              #    1.25  insns per cycle        
                                             #    0.43  stalled cycles per insn [83.25%]
     5,351,057,260 branches                  #  761.593 M/sec                   [83.49%]
       118,150,150 branch-misses             #    2.21% of all branches         [83.15%]

       5.476816754 seconds time elapsed

** Basic Go Profiling

Learn the basics of reading Stack Traces.  
[[https://github.com/ardanlabs/gotraining/blob/master/topics/go/profiling/stack_trace/README.md][Stack Traces and Core Dumps]]

Learn the basics of using GODEBUG.  

[[https://github.com/ardanlabs/gotraining/blob/master/topics/go/profiling/godebug/README.md][GODEBUG]]

Learn the basics of using memory and cpu profiling.  

[[https://github.com/ardanlabs/gotraining/blob/master/topics/go/profiling/memcpu/README.md][Memory and CPU Profiling]]

Learn the basics of using http/pprof.  

[[https://github.com/ardanlabs/gotraining/blob/master/topics/go/profiling/pprof/README.md][pprof Profiling]]

Learn the basics of blocking profiling.  

[[https://github.com/ardanlabs/gotraining/blob/master/topics/go/profiling/blocking/README.md][Blocking Profiling]]


Learn the basics of mutex profiling.  

[[https://github.com/ardanlabs/gotraining/blob/master/topics/go/profiling/mutex/README.md][Mutex Profiling]]


Learn the basics of tracing.    

[[https://github.com/ardanlabs/gotraining/blob/master/topics/go/profiling/trace/README.md][Tracing]]

Learn the basics of profiling and tracing a larger application.  

[[https://github.com/ardanlabs/gotraining/blob/master/topics/go/profiling/project/README.md][Real World Example]]


** Godoc Analysis

The `godoc` tool can help you perform static analysis on your code.

	// Perform a pointer analysis and then run the godoc website.
	$ godoc -analysis pointer -http=:8080

[[https://golang.org/lib/godoc/analysis/help.html][Static analysis features of godoc]] - Go Team

** HTTP Tracing

HTTP tracing facilitate the gathering of fine-grained information throughout the lifecycle of an HTTP client request.

[[https://github.com/ardanlabs/gotraining/blob/master/topics/go/profiling/http_trace/README.md][HTTP Tracing Package]]

All material is licensed under the [[http://www.apache.org/licenses/LICENSE-2.0][Apache License Version 2.0, January 2004]].
