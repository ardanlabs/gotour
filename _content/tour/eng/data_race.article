Data Races
A data race is when two or more goroutines attempt to read and write to the same resource at the same time.

* Data Races

- [[https://www.ardanlabs.com/training/individual-on-demand/ultimate-go-bundle/][Watch The Video]]
- Need Financial Assistance, Use Our [[https://www.ardanlabs.com/scholarship/][Scholarship Form]]

A data race is when two or more Goroutines are trying to access the same memory
location at the same time where at least one Goroutine is performing a write. When
this happens it is impossible to predict the result. These types of bugs are difficult
to find because they cause issues that always appear random.

These ~8 minutes from Scott Meyers is great to listen to here:

[[https://youtu.be/WDIkqP4JbkE?t=1809][CPU Caches and Why You Care 30:09-38:30]]

** Code Review

- *Example* *1:* Data Race
- *Example* *2:* Atomic Increments
- *Example* *3:* Mutex
- *Example* *4:* Read/Write Mutex
- *Example* *5:* Map Data Race
- *Example* *6:* Interface Based Race Condition

.play data_race/example1.go
.play data_race/example2.go     
.play data_race/example3.go
.play data_race/example4.go
.play data_race/example5.go
.play data_race/example6.go

** Data Race Example

This is a great example of a data race and how they can be hidden for years and
eventually show up at odd times and cause data corruption.

    var counter int

    func main() {
        const grs = 2

        var wg sync.WaitGroup
        wg.Add(grs)

        for g := 0; g < grs; g++ {
            go func() {
                for i := 0; i < 2; i++ {
                    value := counter
                    value++
                    counter = value
                }
                wg.Done()
            }()
        }

        wg.Wait()
        fmt.Println("Counter:", counter)
    }

This program creates two Goroutines that each access the same integer variable,
incrementing the variable twice. The Goroutine performs a read, modify, and write
operation against the shared state manually.

    var counter int

    func main() {
        . . .

        go func() {
            for i := 0; i < 2; i++ {
                value := counter
                value++
                counter = value
            }
            wg.Done()
        }()

        . . .
    }

You can see the access to the shared state inside the for loop. When you build and run
this program you get the right answer of 4 each and every time.

    $ ./example1
    Final Counter: 4

    $ ./example1
    Final Counter: 4

    $ ./example1
    Final Counter: 4

How is this working?

    G1                            Shared State: 0                            G2
    ----------------------------------------------------------------------------
    Read:   0
    Modify: 1
    Write:  1                         Shared State: 1
    Context Switch 
                                                                      Read: 1
                                                                    Modify: 2
                                    Shared State: 2                  Write: 2
                                                               Context Switch 
    Read:   2
    Modify: 3
    Write:  3                         Shared State: 3
    Terminate
                                                                      Read: 3
                                                                    Modify: 4
                                    Shared State: 4                  Write: 4
                                                                    Terminate
    ----------------------------------------------------------------------------

The read, modify and write operations are happening uninterrupted. Just because I
am getting the right answer doesn’t mean there isn’t a problem. What happens if
you add a log statement in the middle of the read, modify, and write operation?

    var counter int

    func main() {
        . . .

        go func() {
            for i := 0; i < 2; i++ {
                value := counter
                value++
                log.Println("logging")     <-- Add Logging Here
                counter = value
            }
            wg.Done()
        }()

        . . .
    }

If you run this program you no longer get the same result of 4, now you get the answer of 2.

    $ ./example1
    Final Counter: 2

    $ ./example1
    Final Counter: 2

    $ ./example1
    Final Counter: 2


What is happening? You are running into a data race bug that did exist before, but
wasn’t happening. The call to log is now causing the scheduler to make a context
switch between the two Goroutines at a bad time.

    G1                                Shared State: 0                         G2
    ----------------------------------------------------------------------------
    Read:   0
    Modify: 1
    Context Switch
                                                                        Read:   0
                                                                        Modify: 1
                                                                   Context Switch 
    Write:  1                         Shared State: 1
    Read:   1
    Modify: 2
    Context Switch
                                    Shared State: 1                     Write:  1
                                                                        Read:   1
                                                                        Modify: 2
                                                                    Context Switch 
    Write:  2                         Shared State: 2
    Terminate
                                    Shared State: 2                     Write:  2
                                                                        Terminate
    ----------------------------------------------------------------------------

After the modify operation a context switch is taking place. The three operations
are no longer uninterrupted and Goroutine 2 ends up with its local value being wrong
by the time it completes the write operation. You are very lucky this is happening every
time and you can see it. But normally a data race like this happens "randomly" and is
impossible to know about until it’s too late. Luckily Go has a race detector to help
find data races.

** Race Detection

There are several ways to engage the race detector. You can use it with the run, build
and test command. If you use it with the build command, you have to remember to run the
program. They say an instrumented binary can slow my program down by ~20%.

    $ go build -race
    $ ./example1

The -race flag is how to instrument the build with the race detector. You will
probably use it more with "go test", but for this example you are instrumenting the
binary and then running it.

    2021/02/01 17:30:52 logging
    2021/02/01 17:30:52 logging
    2021/02/01 17:30:52 logging
    ==================
    WARNING: DATA RACE
    Write at 0x000001278d88 by goroutine 8:
    main.main.func1()
        /data_race/example1/example1.go:41 +0xa6

    Previous read at 0x000001278d88 by goroutine 7:
    main.main.func1()
        /data_race/example1/example1.go:38 +0x4a

    Goroutine 8 (running) created at:
    main.main()
        /data_race/example1/example1.go:36 +0xaf

    Goroutine 7 (finished) created at:
    main.main()
        /data_race/example1/example1.go:36 +0xaf
    ==================
    2021/02/01 17:30:52 logging
    Final Counter: 2
    Found 1 data race(s)

You can see a race was detected when running the program. This would happen with or
without the log statement inserted. When a race is detected, the program panics and
provides this trace. The trace shows where there was unsynchronized access to the
same shared state where at least one access was a write.

In this trace, a Goroutine performed a write at address 0x000001278d88 on line 41,
and there was an unsynchronized read at the same address by another Goroutine on
line 38. Both Goroutines were created on line 36.

    36 go func() {
    37     for i := 0; i < 2; i++ {
    38         value := counter
    39         value++
    40         log.Println("logging")
    41         counter = value
    42     }
    43     wg.Done()
    44 }()

You can clearly see the unsynchronized read and write. As a side note, the plus plus
operation on line 39 would also be a data race if the code was accessing the counter
variable. The plus plus operation is a read, modify, and write operation underneath
and the operating system could easily context switch in the middle of that.

So how can you fix the code to make sure that you remove the data race? There are two
tools you can use, atomic instructions and mutexes.

** Atomics

Atomics provide synchronization at the hardware level. Because of this, it’s limited
to words and half-words of data. So they’re great for counters or fast switching
mechanics. The WaitGroup API’s use atomics.

What changes do you need to make to apply atomics to the code?

    var counter int32                             <-- CHANGED

    func main() {
        const grs = 2

        var wg sync.WaitGroup
        wg.Add(grs)

        for g := 0; g < grs; g++ {
            go func() {
                for i := 0; i < 2; i++ {
                    atomic.AddInt32(&counter, 1)  <-- CHANGED
                }
                wg.Done()
            }()
        }

        wg.Wait()
        fmt.Println("Counter:", counter)
    }

You only need to do a couple things. First, change the counter variable to be a
precision based integer. You can see that at the top of the code listing. The atomic
functions only work with precision based integers. Second, remove the manually
read, modify, and write code for one call to atomic.AddInt32. That one call handles
it all.

All of the functions associated with the atomic package take the address to the
shared state to be synchronized. Synchronization only happens at the address level.
So different Goroutines calling the same function, but at a different address, won’t
be synchronized.

The API for atomics looks like this:

    func AddInt32(addr *int32, delta int32) (new int32)
    func AddInt64(addr *int64, delta int64) (new int64)
    func AddUint32(addr *uint32, delta uint32) (new uint32)
    func AddUint64(addr *uint64, delta uint64) (new uint64)
    func AddUintptr(addr *uintptr, delta uintptr) (new uintptr)

    func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool)
    func CompareAndSwapInt64(addr *int64, old, new int64) (swapped bool)
    func CompareAndSwapPointer(addr *unsafe.Pointer, old, new unsafe.Pointer) (swapped bool)
    func CompareAndSwapUint32(addr *uint32, old, new uint32) (swapped bool)
    func CompareAndSwapUint64(addr *uint64, old, new uint64) (swapped bool)
    func CompareAndSwapUintptr(addr *uintptr, old, new uintptr) (swapped bool)

    func LoadInt32(addr *int32) (val int32)
    func LoadInt64(addr *int64) (val int64)
    func LoadPointer(addr *unsafe.Pointer) (val unsafe.Pointer)
    func LoadUint32(addr *uint32) (val uint32)
    func LoadUint64(addr *uint64) (val uint64)
    func LoadUintptr(addr *uintptr) (val uintptr)

    func StoreInt32(addr *int32, val int32)
    func StoreInt64(addr *int64, val int64)
    func StorePointer(addr *unsafe.Pointer, val unsafe.Pointer)
    func StoreUint32(addr *uint32, val uint32)
    func StoreUint64(addr *uint64, val uint64)
    func StoreUintptr(addr *uintptr, val uintptr)

    func SwapInt32(addr *int32, new int32) (old int32)
    func SwapInt64(addr *int64, new int64) (old int64)
    func SwapPointer(addr *unsafe.Pointer, new unsafe.Pointer) (old unsafe.Pointer)
    func SwapUint32(addr *uint32, new uint32) (old uint32)
    func SwapUint64(addr *uint64, new uint64) (old uint64)
    func SwapUintptr(addr *uintptr, new uintptr) (old uintptr)

    type Value
        func (v *Value) Load() (x interface{})
        func (v *Value) Store(x interface{})

You can see that the first parameter is always the address to a precision based
integer or pointer. There is also a type named Value that provides a synchronous
value with a small API.

** Mutexes

What if you wanted to keep the three lines of code you had. Then atomics aren’t going to
work. What you need then is a mutex. A mutex lets me box a group of code so only one
Goroutine at a time can execute that code.

    var counter int

    func main() {
        const grs = 2

        var wg sync.WaitGroup
        wg.Add(grs)

        var mu sync.Mutex                    <-- CHANGED

        for g := 0; g < grs; g++ {
            go func() {
                for i := 0; i < 2; i++ {
                    mu.Lock()                <-- CHANGED
                    {
                        value := counter
                        value++
                        counter = value
                    }
                    mu.Unlock()              <-- CHANGED
                }
                wg.Done()
            }()
        }

        wg.Wait()
        fmt.Println("Counter:", counter)
    }

There are several changes to this code from the original. You added the construction
of the mu variable to be a mutex set to its zero value. Then inside the for loop,
you added calls to Lock and Unlock with an artificial code block. Inside the code block
you have the code that needs to be synchronized. The code block is used for readability.

With this code in place, the scheduler will only allow one Goroutine to enter the
code block at a time. It’s important to understand that a mutex is not a queue.
The first Goroutine that calls Lock isn’t necessarily the first Goroutine who gets
the Lock. There is a fairness based algorithm but this is done on purpose so people
don’t use mutexes as queues.

It’s important to remember the Lock creates back pressure, so the longer it takes to
get from the Lock to the Unlock, the more chance of Goroutines waiting for their turn.
If you forget to call Unlock, then all Goroutines waiting will deadlock. This is why it’s
critical that the call to Lock and Unlock happen in the same function. Make sure I’m
doing the bare minimum synchronization you need in the code block, but at least the
minimum.

This is very bad code where someone is trying to get in and out of the Lock so quickly
they actually lose the synchronization and the race detector can’t even discover the
problem.

    var counter int

    func main() {
        const grs = 2

        var wg sync.WaitGroup
        wg.Add(grs)

        var mu sync.Mutex

        for g := 0; g < grs; g++ {
            go func() {
                for i := 0; i < 2; i++ {
                    var value int
                    mu.Lock()              <-- Bad Use Of Mutex
                    {
                        value = counter
                    }
                    mu.Unlock()

                    value++

                    mu.Lock()              <-- Bad Use Of Mutex
                    {
                        counter = value
                    }
                    mu.Unlock()
                }
                wg.Done()
            }()
        }

        wg.Wait()
        fmt.Println("Counter:", counter)
    }

As a general guideline, if you see a call to Lock from the same mutex twice in the same
function, stop the code review. There is probably a mistake or over complication. In
this case the calls to read and write are being synchronized, however, two Goroutines
can end up at the value++ line of code with the same value. The data race still exists
and the race detector is helpless in finding it.

** Read/Write Mutexes

There is a second type of mutex called a read/write mutex. It allows me to separate
the locks around reads and writes. This is important since reading data doesn’t pose
a threat unless a Goroutine is attempting to write at the same time. So this type of
mutex allows multiple Goroutines to read the same memory at the same time. As soon
as a write lock is requested, the reads are no longer issued, the write takes place,
the reads can start again.

    package main

    import (
        "fmt"
        "math/rand"
        "sync"
        "time"
    )

    var data []string
    var rwMutex sync.RWMutex

    func main() {
        var wg sync.WaitGroup
        wg.Add(1)

        go func() {
            for i := 0; i < 10; i++ {
                writer(i)
            }
            wg.Done()
        }()

        for i := 0; i < 8; i++ {
            go func(id int) {
                for {
                    reader(id)
                }
            }(i)
        }

        wg.Wait()
        fmt.Println("Program Complete")
    }

    func writer(i int) {
        rwMutex.Lock()
        {
            time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)
            fmt.Println("****> : Performing Write")
            data = append(data, fmt.Sprintf("String: %d", i))
        }
        rwMutex.Unlock()
    }

    func reader(id int) {
        rwMutex.RLock()
        {
            time.Sleep(time.Duration(rand.Intn(10)) * time.Millisecond)
            fmt.Printf("%d : Performing Read : Length[%d]\n", id, len(data))
        }
        rwMutex.RUnlock()
    }

You can see the use of a read/write mutex where there are 8 Goroutines reading the
length of a slice within a 10 millisecond delay of each other, and 1 Goroutine waking
up within 100 milliseconds to append a value (write) to the slice.

The key is the implementation of the writer and reader functions. Notice how you use
Lock for the writer and RLock for the reader. One of the biggest mistakes you can
make with this is mixing up the Unlock calls with the wrong version. Having a Lock
with a RUnlock will never end well.

    7 : Performing Read : Length[0]
    5 : Performing Read : Length[0]
    0 : Performing Read : Length[0]
    3 : Performing Read : Length[0]
    7 : Performing Read : Length[0]
    2 : Performing Read : Length[0]
    1 : Performing Read : Length[0]
    ****> : Performing Write
    0 : Performing Read : Length[1]
    5 : Performing Read : Length[1]
    3 : Performing Read : Length[1]
    6 : Performing Read : Length[1]
    7 : Performing Read : Length[1]
    4 : Performing Read : Length[1]
    1 : Performing Read : Length[1]
    2 : Performing Read : Length[1]
    ****> : Performing Write
    7 : Performing Read : Length[2]
    1 : Performing Read : Length[2]
    3 : Performing Read : Length[2]

The output shows how multiple Goroutines are reading at the same time, but all
the reading stops when the write takes place.

** Notes

- Goroutines need to be coordinated and synchronized.
- When two or more goroutines attempt to access the same resource, we have a data race.
- Atomic functions and mutexes can provide the support we need.

** Cache Coherency and False Sharing

This content is provided by Scott Meyers from his talk in 2014 at Dive:

[[https://youtu.be/WDIkqP4JbkE?t=1809][CPU Caches and Why You Care (30:09-38:30)]]  
[[https://github.com/ardanlabs/gotraining/blob/master/topics/go/testing/benchmarks/falseshare/README.md][Code Example]]

.image /tour/eng/static/img/figure1_data_race.png

** Cache Coherency and False Sharing Notes

- Thread memory access matters.
- If your algorithm is not scaling look for false sharing problems.

** Extra Reading

- [[http://www.drdobbs.com/parallel/eliminate-false-sharing/217500206][Eliminate False Sharing]] - Herb Sutter    
- [[https://golang.org/ref/mem][The Go Memory Model]]    
- [[http://blog.golang.org/race-detector][Introducing the Go Race Detector]] - Dmitry Vyukov and Andrew Gerrand    
- [[https://www.ardanlabs.com/blog/2013/09/detecting-race-conditions-with-go.html][Detecting Race Conditions With Go]] - William Kennedy    
- [[https://golang.org/doc/articles/race_detector.html][Data Race Detector]]    

* Exercises

Use the template as a starting point to complete the exercises. A possible solution is provided.

** Exercise 1

Given the following program, use the race detector to find and correct the data race.

.play data_race/exercise1.go
.play data_race/answer1.go
