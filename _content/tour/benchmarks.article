Benchmarking
In this section, I will learn how to write benchmarks in Go and use the integrated support provided by the language. I will also learn how to realize that benchmarks lie and I need to be careful when interpreting the results.

* Benchmarking

Benchmarks lie! At the same time, until I run a benchmark I’m only guessing. If there’s one thing about Go, it’s that I never have to guess. That’s how good the tooling is.

The standard library and the Go frontend tooling has everything I need to write a benchmark. It all starts with creating a file with the _test.go naming convention inside the package I want to benchmark, and then adding benchmark functions by using the word Benchmark with a capital B for each function.

** Package Review

- *Basic* *Test:* Basic Benchmarking
.play benchmarks/basic/basic_test.go
  
- *Sub* *Test:* Sub Benchmarks
.play benchmarks/sub/sub_test.go
  
- *Validate* *Test:* Validate Benchmarks
.play benchmarks/validate/validate_test.go

Listing 1

    sample_test.go

    package sample

    import (
        "testing"
    )

    func BenchmarkDownload(b *testing.B) {}
    func BenchmarkUpload(b *testing.B) {}

These are examples of benchmark functions I could declare in the sample_test.go testing file. It’s important that the first letter following the word Benchmark in the function name starts with a capital letter. If I don’t, the testing tool won’t see the function as a benchmark function. The other important piece is that the benchmark function takes a testing.B pointer as the only argument.

Here is a super interesting benchmark to run.

Listing 2

    package basic

    import (
        "fmt"
        "testing"
    )

    var gs string

    func BenchmarkSprint(b *testing.B) {
        var s string
        for i := 0; i < b.N; i++ {
            s = fmt.Sprint("hello")
        }
        gs = s
    }

    func BenchmarkSprintf(b *testing.B) {
        var s string
        for i := 0; i < b.N; i++ {
            s = fmt.Sprintf("hello")
        }
        gs = s
    }

The premise of this benchmark is to know which version of Sprint is faster, the regular or format version. When people are asked to guess, they tend to say the regular version since there is no formatting necessary. These people are always wrong.

Listing 3

    $ go test -bench . -benchtime 3s -benchmem

    goos: darwin
    goarch: amd64
    pkg: github.com/ardanlabs/gotraining/topics/go/testing/benchmarks/basic
    cpu: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz

    BenchmarkSprint-16     56956252     55.48 ns/op     5 B/op     1 allocs/op
    BenchmarkSprintf-16    80984947     42.46 ns/op     5 B/op     1 allocs/op

I can see from the results that the format version is faster by 13 nanoseconds per operation. The difference is insignificant, but nonetheless, there is a difference.

I want to break down both the writing and running of these benchmarks.


Listing 4

    var gs string

    func BenchmarkSprint(b *testing.B) {
        var s string

        for i := 0; i < b.N; i++ {
            s = fmt.Sprint("hello")
        }

        gs = s
    }

At the core of every benchmark is the for loop from 0 to b.N. Inside this loop is where the code to be benchmarked is placed. To understand the loop, I need to understand a setting called -benchtime.

The -benchtime setting represents the total amount of time to spin the loop before providing a result. The default -benchtime is 1 second. This is where things get interesting because I can’t spin a loop based on time, only on a number of iterations. The number of iterations required to match the -benchtime needs to be identified.

Identifying the correct b.N to match the -benchtime is accomplished through some trial and error. At the very beginning of running the benchmark, the tooling will set the value of b.N to 1 and run the loop. Then it will multiply the value of b.N by 100 until it gets close to the -benchtime. Then the algorithm can fix on a working b.N.

Listing 5

    var gs string
    var a []int

    func BenchmarkSprint(b *testing.B) {
        var s string

        a = append(a, b.N)
        for i := 0; i < b.N; i++ {
            s = fmt.Sprint("hello")
        }

        if len(a) > 4 {
            fmt.Println(a)
        }

        gs = s
    }

I added the ability to capture the value of b.N on every call to the benchmark function. Knowing that in this case it will take 5 calls to find the right b.N, that is when I output the contents of the slice.


Listing 6

    BenchmarkSprint-16     	[1 100 10000 1000000 54495168]

I can see the tooling identifying a value of b.N after 5 tries, finding the right number after trying a million iterations.

Another important aspect of benchmarking is the compiler. The compiler will build this code into a test binary to run the benchmark. It’s important that the code inside the loop is accurate to how it runs in production. Any slight variation could change the compiler's behavior on how the code is built, or how the code behaves at runtime. 

Back to the original code for the Sprint benchmark.

Listing 7

    var gs string

    func BenchmarkSprint(b *testing.B) {
        var s string

        for i := 0; i < b.N; i++ {
            s = fmt.Sprint("hello")
        }

        gs = s
    }

This benchmark is measuring the performance of calling Sprint. Notice how the Sprint function returns a new string after the call. It’s important to capture that return value as part of running the benchmark because doing so represents the real behavior of using this function in production code.

If I don’t capture the return value from the call to Sprint, it’s possible that the compiler could throw the function call out of the compiled binary. The function is useless without capturing the return since the behavior of the call alone wouldn’t change the behavior of the program. If the compiler chooses to throw the call to Sprint away, the benchmark would just spin an empty loop, providing an inaccurate result.

When I run the benchmark, I am using the following call on the command line.

Listing 8

    $ go test -bench . -benchtime 3s -benchmem

Breaking this down, I am asking go test to run all the benchmark functions it finds in the current directory (-bench .), increasing the time of spinning the loop to three seconds (-benchtime 3s), and to show memory allocations (-benchmem).

Listing 9

    BenchmarkSprint-16     56956252     55.48 ns/op     5 B/op     1 allocs/op
    BenchmarkSprintf-16    80984947     42.46 ns/op     5 B/op     1 allocs/op

    BenchmarkSprint-16 	Name of the benchmark function and the number of 
                    threads which was 16.

    56956252			The number of iterations of the loop that were executed
                    which was 56,956,252.

    55.48 ns/op			The amount of time the code inside the loop took to
                    execute which was 55.48 nanoseconds.

    5 B/op			The amount of memory the code inside the loop allocated
                    which was 5 bytes.

    1 allocs/op			The number of values the code inside the loop allocated
                    which was 1 value.

In the end, the use of Sprintf was faster than Sprint for the string hello, though the both allocated the same number of values and total amount of memory on the heap.
8.2 Basic Sub-Benchmarks
There are times when I may want to group a set of benchmarks together under a single benchmark function. I might have a data table that I want to use to control which data is being benchmarked on the command line. If I fall into any of these use cases, the sub-benchmarking support in Go is what I need.

Listing 10

    package basic

    import (
        "fmt"
        "testing"
    )

    var gs string

    func BenchmarkSprint(b *testing.B) {
        b.Run("none", benchSprint)
        b.Run("format", benchSprintf)
    }

A quick way to show this is to rename the existing benchmark functions to benchSprint and benchSprintf, then write a single benchmark function that uses the b.Run function.

Now on the command line I can control what runs.

Listing 11

    $ go test -bench .
    $ go test -bench BenchmarkSprint/none
    $ go test -bench BenchmarkSprint/format

This is how I can run all the sub-benchmarks or specify any individual sub-benchmark.
8.3 Validate Benchmarks
When I started this chapter, I tried to make it clear that benchmarks lie and I must validate the results. Especially when the results are not what I expected. To show this, I have written a merge sort algorithm so I can run the algorithm under different conditions. First with a single Goroutine, then using a different Goroutine for every split of the collection that needs to be sorted, finally only using the same number of Goroutines that can be run in parallel.

Listing 12

    func merge(l, r []int) []int {
        // Declare the sorted return list with the proper capacity.
        ret := make([]int, 0, len(l)+len(r))

        // Compare the number of items required.
        for {
            switch {
            case len(l) == 0:
                // We appended everything in the left list so now append
                // everything contained in the right and return.
                return append(ret, r...)

            case len(r) == 0:
                // We appended everything in the right list so now append
                // everything contained in the left and return.
                return append(ret, l...)

            case l[0] <= r[0]:
                // First value in the left list is smaller than the
                // first value in the right so append the left value.
                ret = append(ret, l[0])

                // Slice that first value away.
                l = l[1:]

            default:
                // First value in the right list is smaller than the
                // first value in the left so append the right value.
                ret = append(ret, r[0])

                // Slice that first value away.
                r = r[1:]
            }
        }
    }

Here is the merge function that performs the work. All this slicing is going to create a large number of allocations on the heap. Merge sort is not a great algorithm to use in Go.

Listing 13

    func single(n []int) []int {
        // Once we have a list of one we can begin to merge values.
        if len(n) <= 1 {
            return n
        }

        // Split the list in half.
        i := len(n) / 2

        // Sort the left side.
        l := single(n[:i])

        // Sort the right side.
        r := single(n[i:])

        // Place things in order and merge ordered lists.
        return merge(l, r)
    }

The single function uses a single Goroutine to perform the sort. It splits the initial list in half, then uses recursion to continue to split the list until the merge function can be executed to sort everything.

Listing 14

    func unlimited(n []int) []int {
        // Once we have a list of one we can begin to merge values.
        if len(n) <= 1 {
            return n
        }

        // Split the list in half.
        i := len(n) / 2

        // Maintain the ordered left and right side lists.
        var l, r []int

        . . .

The unlimited function starts out the same as the single function. Then it begins to throw new Goroutines at each split of the list.


Listing 15

    func unlimited(n []int) []int {    
        . . .

        // For each split we will have 2 goroutines.
        var wg sync.WaitGroup
        wg.Add(2)

        // Sort the left side concurrently.
        go func() {
            l = unlimited(n[:i])
            wg.Done()
        }()

        // Sort the right side concurrently.
        go func() {
            r = unlimited(n[i:])
            wg.Done()
        }()

        // Wait for the splitting to end.
        wg.Wait()

        // Place things in order and merge ordered lists.
        return merge(l, r)
    }

This could result in tens of thousands of Goroutines depending on the size of the list to be sorted.

Listing 16

    func numCPU(n []int) []int {
        // Once we have a list of one we can begin to merge values.
        if len(n) <= 1 {
            return n
        }

        // Split the list in half.
        i := len(n) / 2

        // Maintain the ordered left and right side lists.
        var l, r []int

        . . .

The numCPU function starts out like the unlimited function. Then it must calculate how many concurrent splits can occur for the number of Goroutines that can run in parallel.

Listing 17

    func numCPU(n []int) []int {
        . . .

        // Calculate how many levels deep we can create goroutines.
        // On an 8 core machine we can keep creating goroutines until level 4.
        //      Lvl 0       1  Lists        1  Goroutine
        //      Lvl 1       2  Lists        2  Goroutines
        //      Lvl 2       4  Lists        4  Goroutines
        //      Lvl 3       8  Lists        8  Goroutines
        //      Lvl 4       16 Lists        16 Goroutines

        // On an 8 core machine this will produce the value of 3.
        maxLevel := int(math.Log2(float64(runtime.GOMAXPROCS(0))))

        . . .

Once the number of levels is calculated, the sorting work can begin.

Listing 18

    func numCPU(n []int, lvl int) []int {
        . . .

        // We don't need more goroutines then we have logical processors.
        if lvl <= maxLevel {
            lvl++

            // For each split we will have 2 goroutines.
            var wg sync.WaitGroup
            wg.Add(2)

            // Sort the left side concurrently.
            go func() {
                l = numCPU(n[:i], lvl)
                wg.Done()
            }()

            // Sort the right side concurrently.
            go func() {
                r = numCPU(n[i:], lvl)
                wg.Done()
            }()

            // Wait for the splitting to end.
            wg.Wait()

            // Place things in order and merge ordered lists.
            return merge(l, r)
        }

        // Sort the left and right side on this goroutine.
        l = numCPU(n[:i], lvl)
        r = numCPU(n[i:], lvl)

        // Place things in order and merge ordered lists.
        return merge(l, r)
    }

With this function complete, I can write the benchmark functions and see what version of merge sort is faster.

Listing 19

    package main

    import (
        "math"
        "runtime"
        "sync"
        "testing"
    )

    var n []int

    func init() {
        for i := 0; i < 1_000_000; i++ {
            n = append(n, i)
        }
    }

    func BenchmarkSingle(b *testing.B) {
        for i := 0; i < b.N; i++ {
            single(n)
        }
    }

    func BenchmarkUnlimited(b *testing.B) {
        for i := 0; i < b.N; i++ {
            unlimited(n)
        }
    }

    func BenchmarkNumCPU(b *testing.B) {
        for i := 0; i < b.N; i++ {
            numCPU(n, 0)
        }
    }

Everything I know suggests that numCPU should be the fastest since it is efficiently using the cpu capacity of the machine.

Listing 20  

    $ go test -bench . -benchtime 3s

    goos: darwin
    goarch: amd64
    pkg: github.com/ardanlabs/gotraining/topics/go/testing/benchmarks/validate
    cpu: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz

    BenchmarkSingle-16        52      66837183 ns/op
    BenchmarkUnlimited-16     13     251840589 ns/op
    BenchmarkNumCPU-16        78      46336693 ns/op

The results of the benchmark show that my expectation was correct, numCPU is faster and in this case by 36.2%. But is this actually correct? What happens if I just run the numCPU benchmark in isolation?


Listing 21

    $ go test -bench NumCPU -benchtime 3s

    goos: darwin
    goarch: amd64
    pkg: github.com/ardanlabs/gotraining/topics/go/testing/benchmarks/validate
    cpu: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz

    BenchmarkNumCPU-16        85      38004899 ns/op

When running the benchmark in isolation, the performance of the numCPU function increased. Now numCPU is 55% faster. That is a huge improvement on such a small data set.

Imagine if the single test ended up beating numCPU when all the benchmarks were run together? I would have thought single was faster than numCPU. Funny enough, this was the case in earlier versions of Go.

Why the difference in performance? Because the unlimited benchmark is creating a lot of Goroutines that are still being cleaned up by the time the numCPU benchmark starts running. This extra load on the runtime is not allowing the numCPU benchmark to be accurate. Once I run the benchmark in isolation, that extra load doesn’t exist and can’t affect the results.

Rule number one of running a benchmark is that the machine needs to be idle.


- [[https://github.com/ardanlabs/gotraining/blob/master/topics/go/testing/benchmarks/prediction/README.md][Prediction]]  
- [[https://github.com/ardanlabs/gotraining/blob/master/topics/go/testing/benchmarks/caching/README.md][Caching]]  
- [[https://github.com/ardanlabs/gotraining/blob/master/topics/go/testing/benchmarks/falseshare/README.md][False Sharing]]

Look at the profiling topic to learn more about using benchmarks to [[/tour/profiling][profile]] code.

** Links

- [[https://dave.cheney.net/2013/06/30/how-to-write-benchmarks-in-go][How to write benchmarks in Go]] - Dave Cheney    
- [[https://www.youtube.com/watch?v=xxDZuPEgbBU][Profiling & Optimizing in Go]] - Brad Fitzpatrick    
- [[https://godoc.org/golang.org/x/perf/cmd/benchstat][Benchstat computes and compares statistics about benchmarks]]    
- [[https://speakerdeck.com/mpvl/advanced-testing-concepts-for-go-1-dot-7][Advanced Testing Concepts for Go 1.7]] - Marcel van Lohuizen    

* Exercises


Write three benchmark tests for converting an integer into a string. First use the fmt.Sprintf function, then the strconv.FormatInt function and finally the strconv.Itoa. Identify which function performs the best.


.play benchmarks/exercises/template1/bench_test.go 

.play benchmarks/exercises/exercise1/bench_test.go
 


All material is licensed under the [[http://www.apache.org/licenses/LICENSE-2.0][Apache License Version 2.0, January 2004]].
