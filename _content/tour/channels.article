Channels
Channels allow goroutines to communicate with each other through the use of signaling semantics.

* Channels

It’s important to think of a channel not as a data structure, but as a mechanic for signaling. This goes in line with the idea that I send and receive from a channel, not read and write. If the problem in front of me can’t be solved with signaling, if the word signaling is not coming out of my mouth, I need to question the use of channels.


** Code Review

- *Example* *1:* Basic mechanics
.play channels/example1/example1.go
   

- *Example* *2:* Tennis game
.play channels/example2/example2.go
   

- *Example* *3:* Relay race
.play channels/example3/example3.go

- *Example* *4:* Fan out pattern
.play channels/example4/example4.go
   
- *Example* *5:* Monitor running time
.play channels/example5/example5.go
   
- *Example* *1:* Channel communication ordering
.play channels/advanced/example1/example1.go

The cost of having the guarantee at the signaling level is unknown latency. The sender won’t know how long they need to wait for the receiver to accept the signal. Having to wait for the receiver creates blocking latency. In this case, unknown amounts of blocking latency. The sender has to wait, for an unknown amount of time, until the receiver becomes available to receive the signal.

Waiting for the receiver means mechanically, the receive operation happens before the send. With channels, the receive happens nanoseconds before, but it’s before. This means the receiver takes the signal and then walks away, allowing the sender to now move on with a guarantee.

What if the process can’t wait for an unknown amount of time? What if that kind of latency won’t work? Then the guarantee can’t be at the signaling level, it needs to be outside of it. The mechanics behind this working is that the send now happens before the receive. The sender can perform the signal without needing the receiver to be available. So the sender gets to walk away and not wait. Eventually, I hope, the receiver shows up and takes the signal.

This is reducing latency cost on the send, but it’s creating uncertainty about signals being received and therefore knowing if there are problems upstream with receivers. This can create the process to accept work that never gets started or finished. It could eventually cause massive back pressure and systems to crash.

The second thing to focus on is, do I need to send data with the signal? If the signal requires the transmission of data, then the signaling is a 1 to 1 between Goroutines. If a new Goroutine needs to receive the signal as well, a second signal must be sent.

If data doesn’t need to be transmitted with the signal, then the signal can be a 1 to 1 or 1 to many between Goroutines. Signaling without data is primarily used for cancellation or shutdowns. It’s done by closing the channel.

The third thing to focus on is channel state. A channel can be in 1 of 3 states.

A channel can be in a nil state by constructing the channel to its zero value state. Sends and receives against channels in this state will block. This is good for situations where I want to implement short term stoppages of work.

A channel can be in an open state by using the built-in function make. Sends and receives against channels in this state will work under the following conditions:

Unbuffered Channels:

- Guarantees at the signaling level with the receive happening before send. Sending and receiving Goroutines need to come together in the same space and time for a signal to be processed.

Buffered Channels:

- Guarantees outside of the signaling level with the send happening before the receive. If the buffer is not full, sends can complete else they block. If the buffer is not empty, receives can complete else they block.

A channel can be in a closed state by using the built-in function close. I don’t need to close a channel to release memory, this is for changing the state. Sending on a closed channel will cause a panic, however receiving on a closed channel will return immediately.

With all this information, I can focus on channel patterns. The focus on signaling is important. The idea is, if I need a guarantee at the signaling level or not, based on latency concerns. If I need to transmit data with the signal or not, based on handling cancellations or not. I want to convert the syntax to these semantics.

** Channel Patterns

There are 7 channel patterns that are important to understand since they provide the building blocks to signaling.

*Wait* *For* *Result*

The wait for result pattern is a foundational pattern used by larger patterns like fan out/in. In this pattern, a Goroutine is created to perform some known work and signals their result back to the Goroutine that created them. This allows for the actual work to be placed on a Goroutine that can be terminated or walked away from. 

Listing 1

    func waitForResult() {
        ch := make(chan string)

        go func() {
            time.Sleep(time.Duration(rand.Intn(500)) * time.Millisecond)
            ch <- "data"
            fmt.Println("child : sent signal")
        }()

        d := <-ch
        fmt.Println("parent : recv'd signal :", d)

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

The beginning of this function uses the builtin function make. In this case, an unbuffered channel is being constructed to its open state. It’s better to look at this as a channel that is being constructed to signal string data with guarantees at the signaling level. Which means the sending Goroutine wants a guarantee that the signal being sent has been received.

Once the channel is constructed, a child Goroutine is created to perform work and the parent Goroutine waits to receive a signal with string data. Because there are guarantees at the signaling level, the amount of time the parent Goroutine will need to wait is unknown. It’s the unknown latency cost of this type of channel.

The child Goroutine goes ahead and begins to perform its work immediately. To simulate the unknown latency problem, a sleep with a random number of milliseconds is employed to define the work. Once the work is done, the child Goroutine performs a send with string data. The parent Goroutine is already blocked waiting in a receive.

Since the receive happens nanoseconds before the send, which creates the guarantee, I would think the print call for the receive signal would always appear before the print for the send. But there is no guarantee in what order I will see the print calls execute. I need to remember, both Goroutines are running on their own operating system thread in parallel, the receive is only happening nanoseconds before, after the channel operation, all things are equal again.

*Fan* *Out/In*

The fan out/in pattern uses the wait for result pattern just described.

Listing 2

    func fanOut() {
        children := 2000
        ch := make(chan string, children)

        for c := 0; c < children; c++ {
            go func(child int) {
                time.Sleep(time.Duration(rand.Intn(200)) * time.Millisecond)
                ch <- "data"
                fmt.Println("child : sent signal :", child)
            }(c)
        }

        for children > 0 {
            d := <-ch
            children--
            fmt.Println(d)
            fmt.Println("parent : recv'd signal :", children)
        }

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

The idea of this pattern is to create a Goroutine for each individual piece of work that is pending and can be done concurrently. In this code sample, I am going to create 2000 child Goroutines to perform 2000 individual pieces of work. I am going to use a buffered channel since there is only one receiver and it’s not important to have a guarantee at the signaling level. That will only create extra latency.

Instead, the idea is to move the guarantee to know when all the signals have been received. This will reduce the cost of latency from the channels. That will be done with a counter that is decremented for each received signal until it reaches zero.

A buffered channel of 2000 is constructed, one for each child Goroutine being created. Then in a loop, 2000 child Goroutines are created and they are off to do their work. A random sleep is used to simulate the work and the unknown amount of time it takes to get the work done. The key is that the order of the work is undefined, out of order, execution which also changes each time the program runs. If this is not acceptable, I can’t use concurrency.

Once all the Goroutines are created, the parent Goroutine waits in a receive loop. Eventually as data is signaled into the buffered channel, the parent Goroutine will pick up the data and eventually all the work is received.

I must remember, a fan out is dangerous in a running service since the number of child Goroutines I create for the fan are a multiplier. If I have a service handling 50k requests on 50 thousand Goroutines, and I decide to use a fan out pattern of 10 child Goroutines for some of the requests, in a worse case scenario I would be talking 500k Goroutines existing at the same time. Depending on the resources those child Goroutines needed, I might not have them available at that scale and the back pressure could bring the service down.

*Wait* *For* *Task*

The wait for task pattern is a foundational pattern used by larger patterns like pooling. 

Listing 3

    func waitForTask() {
        ch := make(chan string)

        go func() {
            d := <-ch
            fmt.Println("child : recv'd signal :", d)
        }()

        time.Sleep(time.Duration(rand.Intn(500)) * time.Millisecond)
        ch <- "data"
        fmt.Println("parent : sent signal")

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

At the beginning, the function creates an unbuffered channel so there is a guarantee at the signaling level. This is critically important for pooling so I can add mechanics later if needed to allow for timeouts and cancellation. Once the channel is created, a child Goroutine is created immediately waiting for a signal with data to perform work. The parent Goroutine begins to prepare that work and finally signals the work to the child Goroutine. Since the guarantee is at the signaling level, the child Goroutine doesn’t know how long it needs to wait.

*Pooling*

The pooling pattern uses the wait for task pattern just described. The pooling pattern allows me to manage resource usage across a well defined number of Goroutines. As explained previously, in Go pooling is not needed for efficiency in CPU processing like at the operating system. It’s more important for efficiency in resource usage.

Listing 4

    func pooling() {
        ch := make(chan string)

        g := runtime.GOMAXPROCS(0)
        for c := 0; c < g; c++ {
            go func(child int) {
                for d := range ch {
                    fmt.Printf("child %d : recv'd signal : %s\n", child, d)
                }
                fmt.Printf("child %d : recv'd shutdown signal\n", child)
            }(c)
        }

        const work = 100
        for w := 0; w < work; w++ {
            ch <- "data"
            fmt.Println("parent : sent signal :", w)
        }

        close(ch)
        fmt.Println("parent : sent shutdown signal")

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

In this pattern, a group of child Goroutines are created to service the same channel. There is efficiency in this because the size of the pool dictates the amount of concurrent work happening at the same time. If I have a pool of 16 Goroutines, that could represent 16 files being opened at any given time or the amount of memory needed for 16 Goroutines to perform their work.

The code starts with the creation of an unbuffered channel. It’s critically important that an unbuffered channel is used because without the guarantee at the signaling level, I can’t perform timeouts and cancellation on the send if needed at a later time. The next part of the code decides the number of child Goroutines the pool will contain.

Listing 5

    g := runtime.GOMAXPROCS(0)

The call to runtime.GOMAXPROCS is important in that it queries the runtime (when passing 0 as a parameter) to the number of threads that exist for running Goroutines. The number should always equal the number of cores/hardware_threads that are available to the program. It represents the amount of CPU capacity available to the program. When the size of the pool isn’t obvious, start with this number as a baseline. It won’t be uncommon for this number to provide a reasonable performance benchmark.

The for loop creates the pool of child Goroutines where each child Goroutine sits in a blocking receive call using the for/range mechanics for a channel.

Listing 6

    for c := 0; c < g; c++ {
        go func(child int) {
            for d := range ch {
                fmt.Printf("child %d : recv'd signal : %s\n", child, d)
            }
            fmt.Printf("child %d : recv'd shutdown signal\n", child)
        }(c)
    }

The for range helps to minimize the amount of code I would otherwise need to receive a signal and then shutdown once the channel is closed. Without the for/range mechanics, I would have to write this code.

Listing 7

    for c := 0; c < g; c++ {
        go func( child int) {
            for {
                d, wd := <-ch      <-- CHANGED
                if !wd {           <-- CHANGED
                    break          <-- CHANGED
                }
                fmt.Printf("child %d : recv'd signal : %s\n", child, d)
            }
            fmt.Printf("child %d : recv'd shutdown signal\n", child)
        }(c)
    }

The for/range eliminates 4 extra lines of code and streamlines the mechanics. It’s important to note, it must not matter which of the child Goroutines in the pool are chosen to receive a signal. Depending on the amount of work being signaled, it could be the same child Goroutines over and over while others are never selected.

Then the call to close is executed which will cause the for loops to terminate and stop the program. If the channel being used was a buffered channel, data would flush out of the buffer first before the child Goroutines would receive the close signal.

*Drop*

The drop pattern is an important pattern for services that may experience heavy loads at times and can drop requests when the service reaches a capacity of pending requests. As an example, a DNS service would need to employ this pattern.

Listing 8

    func drop() {
        const cap = 100
        ch := make(chan string, cap)

        go func() {
            for p := range ch {
                fmt.Println("child : recv'd signal :", p)
            }
        }()

        const work = 2000
        for w := 0; w < work; w++ {
            select {
            case ch <- "data":
                fmt.Println("parent : sent signal :", w)
            default:
                fmt.Println("parent : dropped data :", w)
            }
        }

        close(ch)
        fmt.Println("parent : sent shutdown signal")

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

The code starts with the creation of a buffered channel. This is a case where it’s reasonable to have a large buffer. Identifying the capacity value (buffer size) will require work in the lab. I want a number that allows the service to maintain reasonable levels of resource usage and performance when the buffer is full.

Next a child Goroutine using the pooling pattern is created. This child Goroutine is waiting for a signal to receive data to work on. In this example, having only one child Goroutine will cause back pressure quickly on the sending side. One child Goroutine will not be able to process all the work in time before the buffer gets full. Representing the service is at capacity.

Inside the for loop, I see the use of a select statement. The select statement is a blocking call that allows the parent Goroutine to handle multiple channel operations at the same time. Each case represents a channel operation, a send or a receive. However, this select is using the default keyword as well, which turns the select into a non-blocking call.

The key to implementing this pattern is the use of default. If the channel buffer is full, that will cause the case statement to block since the send can’t complete. When every case in a select is blocked, and there is a default, the default is then executed. This is where the drop code is placed.

In the drop code, I can now decide what to do with the request. I can return a 500 to the caller. I could store the request somewhere else. The key is I have options.

*Cancellation*

The cancellation pattern is used to tell a function performing some I/O how long I am willing to wait for the operation to complete. Sometimes I can cancel the operation, and sometimes all I can do is just walk away.

Listing 9

    func cancellation() {
        duration := 150 * time.Millisecond
        ctx, cancel := context.WithTimeout(context.Background(), duration)
        defer cancel()

        ch := make(chan string, 1)

        go func() {
            time.Sleep(time.Duration(rand.Intn(200)) * time.Millisecond)
            ch <- "data"
        }()

        select {
        case d := <-ch:
            fmt.Println("work complete", d)

        case <-ctx.Done():
            fmt.Println("work cancelled")
        }

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

The code starts with defining a time.Duration variable named duration set to 150 milliseconds. Then a Context value is created to support a timeout of the 150 seconds using the WithTimeout function. That function takes a Context value in and returns a new one with the changes. In this case, I use the Background function which returns an empty parent Context.

It’s important to call the cancel function that is returned as the second argument from WithTimeout using a defer. If that cancel function is not called at least once, there will be a memory leak. 

After a buffered channel of 1 is created, a child Goroutine is created to perform some I/O bound work. In this case, a random Sleep call is made to simulate blocking work that can’t be directly cancelled. That work can take up to 200 milliseconds to finish. There is a 50 millisecond difference between the timeout and the amount of time the work could take.

With the child Goroutine created and performing the work, the parent Goroutine blocks in a select statement waiting on two signals. The first case represents the child Goroutine finishing the work on time and the result being received. That is what I want. The second case represents a timeout from the Context. This means the work didn’t finish within the 150 millisecond time limit.
If the parent Goroutine receives the timeout signal, it walks away. In this situation, it can’t inform the child Goroutine that it won’t be around to receive its signal. This is why it’s so important for the work channel to be a buffer of 1. The child Goroutine needs to be able to send its signal, whether or not the parent Goroutine is around to receive it. If a non-buffered channel is used, the child Goroutine will block forever and become a memory leak.

*Fan* *Out/In* *Semaphore*

The fan out/in semaphore pattern provides a mechanic to control the number of Goroutines executing work at any given time while still creating a unique Goroutine for each piece of work.

Listing 10

    func fanOutSem() {
        children := 2000
        ch := make(chan string, children)

        g := runtime.GOMAXPROCS(0)
        sem := make(chan bool, g)

        for c := 0; c < children; c++ {
            go func(child int) {
                sem <- true
                {
                    t := time.Duration(rand.Intn(200)) * time.Millisecond
                    time.Sleep(t)
                    ch <- "data"
                    fmt.Println("child : sent signal :", child)
                }
                <-sem
            }(c)
        }

        for children > 0 {
            d := <-ch
            children--
            fmt.Println(d)
            fmt.Println("parent : recv'd signal :", children)
        }

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

At the start of the function, a channel with a buffer size of 2000 is set. This is the same thing we need in the original fan out/in pattern. One buffer for each child Goroutine that will be created. Then, like the pooling pattern, the use of the GOMAXPROCS function is used to determine how many of the 2000 child Goroutines will be allowed to execute their work at any given time.

With g configured, a second buffered channel is constructed next with a buffer sized to the number of child Goroutines that can execute their work at the same time. This channel is the semaphore that will control the number of child Goroutines performing work.

Then a for loop is used to create all 2000 child Goroutines and each child Goroutine finds itself in a send operation (sem <- true) against the semaphore channel. Here is where the rubber hits the road. Only a GOMAXPROCS number of child Goroutines can perform this send without blocking. The other 2000 - GOMAXPROCS child Goroutines will block until the running child Goroutines get to the receive operation (<-sem).  This code uses a code block to show the code that is being executed between the semaphore locking. I like this for better readability.

At the end of the function, the parent Goroutine waits to receive work from all 2000 child Goroutines. For each piece of work received, the children variable is decremented until it gets down to zero. Just like the original fan out/in pattern. 

*Bounded* *Work* *Pooling*

The bounded work pooling pattern uses a pool of Goroutines to perform a fixed amount of known work.

Listing 11

    func boundedWorkPooling() {
        work := []string{"paper", "paper", "paper", "paper", 2000: "paper"}

        g := runtime.GOMAXPROCS(0)
        var wg sync.WaitGroup
        wg.Add(g)

        ch := make(chan string, g)

        for c := 0; c < g; c++ {
            go func(child int) {
                defer wg.Done()
                for wrk := range ch {
                    fmt.Printf("child %d : recv'd signal : %s\n", child, wrk)
                }
                fmt.Printf("child %d : recv'd shutdown signal\n", child)
            }(c)
        }

        for _, wrk := range work {
            ch <- wrk
        }
        close(ch)
        wg.Wait()

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

Right from the start, the function defines 2000 arbitrary pieces of work to perform. Then the GOMAXPROCS function is used to define the number of child Goroutines to use in the pool and a WaitGroup is constructed to make sure the parent Goroutine can be told to wait until all 2000 pieces of work are completed.

Just like I saw with the pooling pattern, a pool of child Goroutines is created in the loop and they all wait on a receive call using the for range mechanics. One change is the call to Done using a defer when each of the child Goroutines in the pool eventually terminate. This will happen when all the work is completed and this is how the pool will report back to the parent Goroutine they are aware they are not needed any longer.

After the creation of the pool of child Goroutines, A loop is executed by the parent Goroutine to start signaling work into the pool. Once the last piece of work is signaled, the channel is closed. Each of the child Goroutines will receive the closed signal once the signals in the buffer are emptied.

*Retry* *Timeout*

The retry timeout pattern is great when I have to ping something (like a database) which might fail, but I don’t want to fail immediately. I want to retry for a specified amount of time before I fail.

Listing 12

    func retryTimeout(ctx context.Context, retryInterval time.Duration,
                    check func(ctx context.Context) error) {

        for {
            fmt.Println("perform user check call")
            if err := check(ctx); err == nil {
                fmt.Println("work finished successfully")
                return
            }

            fmt.Println("check if timeout has expired")
            if ctx.Err() != nil {
                fmt.Println("time expired 1 :", ctx.Err())
                return
            }

            fmt.Printf("wait %s before trying again\n", retryInterval)
            t := time.NewTimer(retryInterval)

            select {
            case <-ctx.Done():
                fmt.Println("timed expired 2 :", ctx.Err())
                t.Stop()
                return
            case <-t.C:
                fmt.Println("retry again")
            }
        }
    }

The function takes a context for the amount of time the function should attempt to perform work unsuccessfully. It also takes a retry interval that specifies how long to wait between attempts, and finally a function to execute. This function is coded by the caller for the specific work (like pinging the database) that needs to be performed and could fail.
The core of the function runs in an endless loop. The first step in the loop is to run the check function passing in the context so the caller’s function can also respect the context. If that doesn’t fail, the function returns that life is good. If it fails, the code goes on to the next step.

Next the context is checked to see if the amount of time given has expired. If it has, the function returns the timeout error, else it continues to the next step which is to create a timer value. The time value is set to the retry interval. The timer could be created above the for loop and reused, which would be good if this function was going to be running a lot. To simplify the code, a new timer is created every time.

The last step is to block on a select statement waiting to receive one of two signals. The first signal is that the context expires. The second signal is the retry interval expires. In the case of the second signal, the loop is restarted and the process runs again.

*Channel* *Cancellation*

With channel cancellation, I can take an existing channel being used already for cancellation purposes (legacy code) and convert its use with a context, where a context is needed for a future function call.

Listing 13

    func channelCancellation(stop <-chan struct{}) {
        ctx, cancel := context.WithCancel(context.Background())
        defer cancel()

        go func() {
            select {
            case <-stop:
                cancel()
            case <-ctx.Done():
            }
        }()

        func(ctx context.Context) error {
            req, err := http.NewRequestWithContext(
                ctx,
                http.MethodGet,
                "https://www.ardanlabs.com/blog/index.xml",
                nil,
        )
        if err != nil {
            return err
        }

        _, err = http.DefaultClient.Do(req)
        if err != nil {
            return err
        }
        return nil
        }(ctx)
    }

This function accepts a channel typed with the empty struct to signal cancellation. This is code that could be found in Go programs prior to the inclusion of context. A function this function needs to call works with the "new" context package.

A context is created using the Background function for the parent context in the WithCancel call. This returns a new context value that can be cancelled with the returned cancel function.

The key is the creation of the Goroutine that blocks in a select statement waiting on two signals. The first signal is the legacy channel that may be closed by the originator. The second is the context itself, which is important if future functions decide to cancel the context directly. On receiving a stop signal, the cancel function is then executed, cancelling the context for all functions that were passed the context.

As an example, a literal function is declared and executed that performs a web request that supports a context for cancellable I/O.

** Design Guidelines

- Learn about the [[https://github.com/ardanlabs/gotraining/blob/master/topics/go/#channel-design][design guidelines]] for channels.

** Diagrams

Guarantee Of Delivery

The `Guarantee Of Delivery` is based on one question: “Do I need a guarantee that the signal sent by a particular goroutine has been received?”

.image /tour/static/img/guarantee_of_delivery.png

Signaling With Or Without Data

When you are going to signal `with` data, there are three channel configuration options you can choose depending on the type of `guarantee` you need.

.image /tour/static/img/signaling_with_data.png

Signaling without data serves the main purpose of cancellation. It allows one goroutine to signal another goroutine to cancel what they are doing and move on. Cancellation can be implemented using both `unbuffered` and `buffered` channels.

.image /tour/static/img/signaling_without_data.png

State

The behavior of a channel is directly influenced by its current `State`. The state of a channel can be `nil`, `open` or `closed`.

.image /tour/static/img/state.png

** Links

- [[https://www.ardanlabs.com/blog/2017/10/the-behavior-of-channels.html][The Behavior Of Channels]] - William Kennedy  
- [[https://golang.org/ref/mem#tmp_7][Channel Communication]]    
- [[http://blog.golang.org/share-memory-by-communicating][Share Memory By Communicating]] - Andrew Gerrand    
- [[https://www.ardanlabs.com/blog/2014/02/the-nature-of-channels-in-go.html][The Nature Of Channels In Go]] - William Kennedy    
- [[http://matt-welsh.blogspot.com/2010/07/retrospective-on-seda.html][A Retrospective on SEDA]] - Matt Welsh    
- [[https://www.youtube.com/watch?v=KBZlN0izeiY][Understanding Channels]] - Kavya Joshi    

** Buffer Bloat - 2011

- Large buffers prevent timely notification of back pressure.
- They defeat your ability to reduce back pressure in a timely matter.
- They can increase latency not reduce it.
- Use buffered channels to provide a way of maintaining continuity.

- Don't use them just for performance.
- Use them to handle well defined bursts of data.
- Use them to deal with speed of light issues between handoffs.

- [[https://www.youtube.com/watch?v=qbIozKVz73g][Bufferbloat: Dark Buffers in the Internet]]  
- [[http://www.bufferbloat.net/projects/cerowrt/wiki/Bloat-videos][Buffer Bloat Videos]]  

 
* Exercises

Exercise 1

Write a program where two goroutines pass an integer back and forth ten times. Display when each goroutine receives the integer. Increment the integer with each pass. Once the integer equals ten, terminate the program cleanly.


.play channels/exercises/template1/template1.go 

.play channels/exercises/exercise1/exercise1.go
 

Exercise 2

Write a program that uses a fan out pattern to generate 100 random numbers concurrently. Have each goroutine generate a single random number and return that number to the main goroutine over a buffered channel. Set the size of the buffer channel so no send ever blocks. Don't allocate more buffers than you need. Have the main goroutine display each random number it receives and then terminate the program.

.play channels/exercises/template2/template2.go

.play channels/exercises/exercise2/exercise2.go
 

Exercise 3

Write a program that generates up to 100 random numbers concurrently. Do not send all 100 values so the number of sends/receives is unknown.


.play channels/exercises/template3/template3.go 

.play channels/exercises/exercise3/exercise3.go
 

Exercise 4

Write a program that generates up to 100 random numbers concurrently using a worker pool. Reject even values. Instruct the workers to shutdown with 100 odd numbers have been collected.

.play channels/exercises/template4/template4.go
 
.play channels/exercises/exercise4/exercise4.go
 

All material is licensed under the [[http://www.apache.org/licenses/LICENSE-2.0][Apache License Version 2.0, January 2004]].
