Concurrency Patterns

* Concurrency Patterns

There are lots of different patterns we can create with goroutines and channels.

** Code Review

- *Chat.go*   : Package chat implements a basic chat room.
- *Task.go*   : Package task provides a pool of goroutines to perform tasks.
- *Pool.go*   : Package pool manages a user defined set of resources.
- *Logger.go* : Package logger shows a pattern of using a buffer to handle log write

.play patterns/chat/chat.go
.play patterns/chat/main/main.go

.play patterns/task/task.go
.play patterns/task/main/main.go

.play patterns/pool/pool.go
.play patterns/pool/main/main.go

.play patterns/logger/logger.go
.play patterns/logger/main/main.go

** Channel Patterns 

There are 7 channel patterns that are important to understand since they provide the building blocks to signaling.

*Wait* *For* *Result*

The wait for result pattern is a foundational pattern used by larger patterns like fan out/in. In this pattern, a Goroutine is created to perform some known work and signals their result back to the Goroutine that created them. This allows for the actual work to be placed on a Goroutine that can be terminated or walked away from. 

Listing 1

    func waitForResult() {
        ch := make(chan string)

        go func() {
            time.Sleep(time.Duration(rand.Intn(500)) * time.Millisecond)
            ch <- "data"
            fmt.Println("child : sent signal")
        }()

        d := <-ch
        fmt.Println("parent : recv'd signal :", d)

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

The beginning of this function uses the builtin function make. In this case, an unbuffered channel is being constructed to its open state. It’s better to look at this as a channel that is being constructed to signal string data with guarantees at the signaling level. Which means the sending Goroutine wants a guarantee that the signal being sent has been received.

Once the channel is constructed, a child Goroutine is created to perform work and the parent Goroutine waits to receive a signal with string data. Because there are guarantees at the signaling level, the amount of time the parent Goroutine will need to wait is unknown. It’s the unknown latency cost of this type of channel.

The child Goroutine goes ahead and begins to perform its work immediately. To simulate the unknown latency problem, a sleep with a random number of milliseconds is employed to define the work. Once the work is done, the child Goroutine performs a send with string data. The parent Goroutine is already blocked waiting in a receive.

Since the receive happens nanoseconds before the send, which creates the guarantee, I would think the print call for the receive signal would always appear before the print for the send. But there is no guarantee in what order I will see the print calls execute. I need to remember, both Goroutines are running on their own operating system thread in parallel, the receive is only happening nanoseconds before, after the channel operation, all things are equal again.

*Fan* *Out/In*

The fan out/in pattern uses the wait for result pattern just described.

Listing 2

    func fanOut() {
        children := 2000
        ch := make(chan string, children)

        for c := 0; c < children; c++ {
            go func(child int) {
                time.Sleep(time.Duration(rand.Intn(200)) * time.Millisecond)
                ch <- "data"
                fmt.Println("child : sent signal :", child)
            }(c)
        }

        for children > 0 {
            d := <-ch
            children--
            fmt.Println(d)
            fmt.Println("parent : recv'd signal :", children)
        }

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

The idea of this pattern is to create a Goroutine for each individual piece of work that is pending and can be done concurrently. In this code sample, I am going to create 2000 child Goroutines to perform 2000 individual pieces of work. I am going to use a buffered channel since there is only one receiver and it’s not important to have a guarantee at the signaling level. That will only create extra latency.

Instead, the idea is to move the guarantee to know when all the signals have been received. This will reduce the cost of latency from the channels. That will be done with a counter that is decremented for each received signal until it reaches zero.

A buffered channel of 2000 is constructed, one for each child Goroutine being created. Then in a loop, 2000 child Goroutines are created and they are off to do their work. A random sleep is used to simulate the work and the unknown amount of time it takes to get the work done. The key is that the order of the work is undefined, out of order, execution which also changes each time the program runs. If this is not acceptable, I can’t use concurrency.

Once all the Goroutines are created, the parent Goroutine waits in a receive loop. Eventually as data is signaled into the buffered channel, the parent Goroutine will pick up the data and eventually all the work is received.

I must remember, a fan out is dangerous in a running service since the number of child Goroutines I create for the fan are a multiplier. If I have a service handling 50k requests on 50 thousand Goroutines, and I decide to use a fan out pattern of 10 child Goroutines for some of the requests, in a worse case scenario I would be talking 500k Goroutines existing at the same time. Depending on the resources those child Goroutines needed, I might not have them available at that scale and the back pressure could bring the service down.

*Wait* *For* *Task*

The wait for task pattern is a foundational pattern used by larger patterns like pooling. 

Listing 3

    func waitForTask() {
        ch := make(chan string)

        go func() {
            d := <-ch
            fmt.Println("child : recv'd signal :", d)
        }()

        time.Sleep(time.Duration(rand.Intn(500)) * time.Millisecond)
        ch <- "data"
        fmt.Println("parent : sent signal")

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

At the beginning, the function creates an unbuffered channel so there is a guarantee at the signaling level. This is critically important for pooling so I can add mechanics later if needed to allow for timeouts and cancellation. Once the channel is created, a child Goroutine is created immediately waiting for a signal with data to perform work. The parent Goroutine begins to prepare that work and finally signals the work to the child Goroutine. Since the guarantee is at the signaling level, the child Goroutine doesn’t know how long it needs to wait.

*Pooling*

The pooling pattern uses the wait for task pattern just described. The pooling pattern allows me to manage resource usage across a well defined number of Goroutines. As explained previously, in Go pooling is not needed for efficiency in CPU processing like at the operating system. It’s more important for efficiency in resource usage.

Listing 4

    func pooling() {
        ch := make(chan string)

        g := runtime.GOMAXPROCS(0)
        for c := 0; c < g; c++ {
            go func(child int) {
                for d := range ch {
                    fmt.Printf("child %d : recv'd signal : %s\n", child, d)
                }
                fmt.Printf("child %d : recv'd shutdown signal\n", child)
            }(c)
        }

        const work = 100
        for w := 0; w < work; w++ {
            ch <- "data"
            fmt.Println("parent : sent signal :", w)
        }

        close(ch)
        fmt.Println("parent : sent shutdown signal")

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

In this pattern, a group of child Goroutines are created to service the same channel. There is efficiency in this because the size of the pool dictates the amount of concurrent work happening at the same time. If I have a pool of 16 Goroutines, that could represent 16 files being opened at any given time or the amount of memory needed for 16 Goroutines to perform their work.

The code starts with the creation of an unbuffered channel. It’s critically important that an unbuffered channel is used because without the guarantee at the signaling level, I can’t perform timeouts and cancellation on the send if needed at a later time. The next part of the code decides the number of child Goroutines the pool will contain.

Listing 5

    g := runtime.GOMAXPROCS(0)

The call to runtime.GOMAXPROCS is important in that it queries the runtime (when passing 0 as a parameter) to the number of threads that exist for running Goroutines. The number should always equal the number of cores/hardware_threads that are available to the program. It represents the amount of CPU capacity available to the program. When the size of the pool isn’t obvious, start with this number as a baseline. It won’t be uncommon for this number to provide a reasonable performance benchmark.

The for loop creates the pool of child Goroutines where each child Goroutine sits in a blocking receive call using the for/range mechanics for a channel.

Listing 6

    for c := 0; c < g; c++ {
        go func(child int) {
            for d := range ch {
                fmt.Printf("child %d : recv'd signal : %s\n", child, d)
            }
            fmt.Printf("child %d : recv'd shutdown signal\n", child)
        }(c)
    }

The for range helps to minimize the amount of code I would otherwise need to receive a signal and then shutdown once the channel is closed. Without the for/range mechanics, I would have to write this code.

Listing 7

    for c := 0; c < g; c++ {
        go func( child int) {
            for {
                d, wd := <-ch      <-- CHANGED
                if !wd {           <-- CHANGED
                    break          <-- CHANGED
                }
                fmt.Printf("child %d : recv'd signal : %s\n", child, d)
            }
            fmt.Printf("child %d : recv'd shutdown signal\n", child)
        }(c)
    }

The for/range eliminates 4 extra lines of code and streamlines the mechanics. It’s important to note, it must not matter which of the child Goroutines in the pool are chosen to receive a signal. Depending on the amount of work being signaled, it could be the same child Goroutines over and over while others are never selected.

Then the call to close is executed which will cause the for loops to terminate and stop the program. If the channel being used was a buffered channel, data would flush out of the buffer first before the child Goroutines would receive the close signal.

*Drop*

The drop pattern is an important pattern for services that may experience heavy loads at times and can drop requests when the service reaches a capacity of pending requests. As an example, a DNS service would need to employ this pattern.

Listing 8

    func drop() {
        const cap = 100
        ch := make(chan string, cap)

        go func() {
            for p := range ch {
                fmt.Println("child : recv'd signal :", p)
            }
        }()

        const work = 2000
        for w := 0; w < work; w++ {
            select {
            case ch <- "data":
                fmt.Println("parent : sent signal :", w)
            default:
                fmt.Println("parent : dropped data :", w)
            }
        }

        close(ch)
        fmt.Println("parent : sent shutdown signal")

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

The code starts with the creation of a buffered channel. This is a case where it’s reasonable to have a large buffer. Identifying the capacity value (buffer size) will require work in the lab. I want a number that allows the service to maintain reasonable levels of resource usage and performance when the buffer is full.

Next a child Goroutine using the pooling pattern is created. This child Goroutine is waiting for a signal to receive data to work on. In this example, having only one child Goroutine will cause back pressure quickly on the sending side. One child Goroutine will not be able to process all the work in time before the buffer gets full. Representing the service is at capacity.

Inside the for loop, I see the use of a select statement. The select statement is a blocking call that allows the parent Goroutine to handle multiple channel operations at the same time. Each case represents a channel operation, a send or a receive. However, this select is using the default keyword as well, which turns the select into a non-blocking call.

The key to implementing this pattern is the use of default. If the channel buffer is full, that will cause the case statement to block since the send can’t complete. When every case in a select is blocked, and there is a default, the default is then executed. This is where the drop code is placed.

In the drop code, I can now decide what to do with the request. I can return a 500 to the caller. I could store the request somewhere else. The key is I have options.

*Cancellation*

The cancellation pattern is used to tell a function performing some I/O how long I am willing to wait for the operation to complete. Sometimes I can cancel the operation, and sometimes all I can do is just walk away.

Listing 9

    func cancellation() {
        duration := 150 * time.Millisecond
        ctx, cancel := context.WithTimeout(context.Background(), duration)
        defer cancel()

        ch := make(chan string, 1)

        go func() {
            time.Sleep(time.Duration(rand.Intn(200)) * time.Millisecond)
            ch <- "data"
        }()

        select {
        case d := <-ch:
            fmt.Println("work complete", d)

        case <-ctx.Done():
            fmt.Println("work cancelled")
        }

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

The code starts with defining a time.Duration variable named duration set to 150 milliseconds. Then a Context value is created to support a timeout of the 150 seconds using the WithTimeout function. That function takes a Context value in and returns a new one with the changes. In this case, I use the Background function which returns an empty parent Context.

It’s important to call the cancel function that is returned as the second argument from WithTimeout using a defer. If that cancel function is not called at least once, there will be a memory leak. 

After a buffered channel of 1 is created, a child Goroutine is created to perform some I/O bound work. In this case, a random Sleep call is made to simulate blocking work that can’t be directly cancelled. That work can take up to 200 milliseconds to finish. There is a 50 millisecond difference between the timeout and the amount of time the work could take.

With the child Goroutine created and performing the work, the parent Goroutine blocks in a select statement waiting on two signals. The first case represents the child Goroutine finishing the work on time and the result being received. That is what I want. The second case represents a timeout from the Context. This means the work didn’t finish within the 150 millisecond time limit.
If the parent Goroutine receives the timeout signal, it walks away. In this situation, it can’t inform the child Goroutine that it won’t be around to receive its signal. This is why it’s so important for the work channel to be a buffer of 1. The child Goroutine needs to be able to send its signal, whether or not the parent Goroutine is around to receive it. If a non-buffered channel is used, the child Goroutine will block forever and become a memory leak.

*Fan* *Out/In* *Semaphore*

The fan out/in semaphore pattern provides a mechanic to control the number of Goroutines executing work at any given time while still creating a unique Goroutine for each piece of work.

Listing 10

    func fanOutSem() {
        children := 2000
        ch := make(chan string, children)

        g := runtime.GOMAXPROCS(0)
        sem := make(chan bool, g)

        for c := 0; c < children; c++ {
            go func(child int) {
                sem <- true
                {
                    t := time.Duration(rand.Intn(200)) * time.Millisecond
                    time.Sleep(t)
                    ch <- "data"
                    fmt.Println("child : sent signal :", child)
                }
                <-sem
            }(c)
        }

        for children > 0 {
            d := <-ch
            children--
            fmt.Println(d)
            fmt.Println("parent : recv'd signal :", children)
        }

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

At the start of the function, a channel with a buffer size of 2000 is set. This is the same thing we need in the original fan out/in pattern. One buffer for each child Goroutine that will be created. Then, like the pooling pattern, the use of the GOMAXPROCS function is used to determine how many of the 2000 child Goroutines will be allowed to execute their work at any given time.

With g configured, a second buffered channel is constructed next with a buffer sized to the number of child Goroutines that can execute their work at the same time. This channel is the semaphore that will control the number of child Goroutines performing work.

Then a for loop is used to create all 2000 child Goroutines and each child Goroutine finds itself in a send operation (sem <- true) against the semaphore channel. Here is where the rubber hits the road. Only a GOMAXPROCS number of child Goroutines can perform this send without blocking. The other 2000 - GOMAXPROCS child Goroutines will block until the running child Goroutines get to the receive operation (<-sem).  This code uses a code block to show the code that is being executed between the semaphore locking. I like this for better readability.

At the end of the function, the parent Goroutine waits to receive work from all 2000 child Goroutines. For each piece of work received, the children variable is decremented until it gets down to zero. Just like the original fan out/in pattern. 

*Bounded* *Work* *Pooling*

The bounded work pooling pattern uses a pool of Goroutines to perform a fixed amount of known work.

Listing 11

    func boundedWorkPooling() {
        work := []string{"paper", "paper", "paper", "paper", 2000: "paper"}

        g := runtime.GOMAXPROCS(0)
        var wg sync.WaitGroup
        wg.Add(g)

        ch := make(chan string, g)

        for c := 0; c < g; c++ {
            go func(child int) {
                defer wg.Done()
                for wrk := range ch {
                    fmt.Printf("child %d : recv'd signal : %s\n", child, wrk)
                }
                fmt.Printf("child %d : recv'd shutdown signal\n", child)
            }(c)
        }

        for _, wrk := range work {
            ch <- wrk
        }
        close(ch)
        wg.Wait()

        time.Sleep(time.Second)
        fmt.Println("-------------------------------------------------")
    }

Right from the start, the function defines 2000 arbitrary pieces of work to perform. Then the GOMAXPROCS function is used to define the number of child Goroutines to use in the pool and a WaitGroup is constructed to make sure the parent Goroutine can be told to wait until all 2000 pieces of work are completed.

Just like I saw with the pooling pattern, a pool of child Goroutines is created in the loop and they all wait on a receive call using the for range mechanics. One change is the call to Done using a defer when each of the child Goroutines in the pool eventually terminate. This will happen when all the work is completed and this is how the pool will report back to the parent Goroutine they are aware they are not needed any longer.

After the creation of the pool of child Goroutines, A loop is executed by the parent Goroutine to start signaling work into the pool. Once the last piece of work is signaled, the channel is closed. Each of the child Goroutines will receive the closed signal once the signals in the buffer are emptied.

*Retry* *Timeout*

The retry timeout pattern is great when I have to ping something (like a database) which might fail, but I don’t want to fail immediately. I want to retry for a specified amount of time before I fail.

Listing 12

    func retryTimeout(ctx context.Context, retryInterval time.Duration,
                    check func(ctx context.Context) error) {

        for {
            fmt.Println("perform user check call")
            if err := check(ctx); err == nil {
                fmt.Println("work finished successfully")
                return
            }

            fmt.Println("check if timeout has expired")
            if ctx.Err() != nil {
                fmt.Println("time expired 1 :", ctx.Err())
                return
            }

            fmt.Printf("wait %s before trying again\n", retryInterval)
            t := time.NewTimer(retryInterval)

            select {
            case <-ctx.Done():
                fmt.Println("timed expired 2 :", ctx.Err())
                t.Stop()
                return
            case <-t.C:
                fmt.Println("retry again")
            }
        }
    }

The function takes a context for the amount of time the function should attempt to perform work unsuccessfully. It also takes a retry interval that specifies how long to wait between attempts, and finally a function to execute. This function is coded by the caller for the specific work (like pinging the database) that needs to be performed and could fail.
The core of the function runs in an endless loop. The first step in the loop is to run the check function passing in the context so the caller’s function can also respect the context. If that doesn’t fail, the function returns that life is good. If it fails, the code goes on to the next step.

Next the context is checked to see if the amount of time given has expired. If it has, the function returns the timeout error, else it continues to the next step which is to create a timer value. The time value is set to the retry interval. The timer could be created above the for loop and reused, which would be good if this function was going to be running a lot. To simplify the code, a new timer is created every time.

The last step is to block on a select statement waiting to receive one of two signals. The first signal is that the context expires. The second signal is the retry interval expires. In the case of the second signal, the loop is restarted and the process runs again.

*Channel* *Cancellation*

With channel cancellation, I can take an existing channel being used already for cancellation purposes (legacy code) and convert its use with a context, where a context is needed for a future function call.

Listing 13

    func channelCancellation(stop <-chan struct{}) {
        ctx, cancel := context.WithCancel(context.Background())
        defer cancel()

        go func() {
            select {
            case <-stop:
                cancel()
            case <-ctx.Done():
            }
        }()

        func(ctx context.Context) error {
            req, err := http.NewRequestWithContext(
                ctx,
                http.MethodGet,
                "https://www.ardanlabs.com/blog/index.xml",
                nil,
        )
        if err != nil {
            return err
        }

        _, err = http.DefaultClient.Do(req)
        if err != nil {
            return err
        }
        return nil
        }(ctx)
    }

This function accepts a channel typed with the empty struct to signal cancellation. This is code that could be found in Go programs prior to the inclusion of context. A function this function needs to call works with the "new" context package.

A context is created using the Background function for the parent context in the WithCancel call. This returns a new context value that can be cancelled with the returned cancel function.

The key is the creation of the Goroutine that blocks in a select statement waiting on two signals. The first signal is the legacy channel that may be closed by the originator. The second is the context itself, which is important if future functions decide to cancel the context directly. On receiving a stop signal, the cancel function is then executed, cancelling the context for all functions that were passed the context.

As an example, a literal function is declared and executed that performs a web request that supports a context for cancellable I/O.

** Generics

Next, I wanted to explore how the Go team could add a package of concurrency patterns into the standard library thanks to generics. This would require declaring channels and functions using generic types.

Listing 14

    type workFn[Result any] func(context.Context) Result

In this example, I declare a type that represents a function which accepts a context and returns a value of generic type Result. This function declaration describes a function that implements the concurrent work that will be performed and the result of that work.

Listing 15

    func doWork[Result any](ctx context.Context, work workFn[Result]) chan Result {
    ch := make(chan Result, 1)

    go func() {
        ch <- work(ctx)
        fmt.Println("doWork : work complete")
    }()

    return ch
    }

Now I write a function named doWork that executes the specified work function concurrently and returns a channel so the caller can receive the result of the work performed by the work function. A generic type named Result is declared to represent the return type for the work function and the type for the channel.

In the implementation of the doWork function, a buffered channel of one is constructed of generic type Result. That’s the channel returned to the caller to receive the result of the concurrent work. In the middle of the function, a goroutine is constructed to execute the work function concurrently. Once the work function returns, the return argument is sent back to the caller through the channel.

To test the use of the doWork function, I built a small program. 

Listing 16

    func main() {
        duration := 100 * time.Millisecond
        ctx, cancel := context.WithTimeout(context.Background(), duration)
        defer cancel()

        dwf := func(ctx context.Context) string {
            time.Sleep(time.Duration(rand.Intn(200)) * time.Millisecond)
            return "work complete"
        }
        result := doWork(ctx, dwf)

        select {
        case v := <-result:
            fmt.Println("main:", v)
        case <-ctx.Done():
            fmt.Println("main: timeout")
        }
    }

Output:

    doWork : work complete
    main: work complete

The program starts by declaring a context that will timeout in 100 milliseconds. Then a work function is declared that waits for up to 200 milliseconds before returning the string, "work complete". With the context and the work function in place, a call to doWork is made and a channel of type string is returned and assigned to the variable result.

The compiler is able to determine the concrete type to use for the generic type Result by inspecting the return type of the literal work function that is passed into the doWork function. This is brilliant because it means I didn’t have to pass the type in on the call to doWork.

With the channel of type string assigned to the variable result, a select case is used to wait for the result to be returned on time, or for the timeout to occur. The doWork function can be used to perform this concurrent work for any concrete type required.

This same idea could be applied to a pool of goroutines that could execute work on a generic input and return a generic result.

Listing 17

    type workFn[Input any, Result any] func(input Input) Result

In this example, I changed the function type to accept a generic input and return a generic result.

Listing 18

    func poolWork[Input any, Result any](
        size int,
        work workFn[Input, Result],
    ) (chan Input, func()) {

        var wg sync.WaitGroup
        wg.Add(size)

        ch := make(chan Input)

        for i := 0; i < size; i++ {
            go func() {
                defer wg.Done()
                for input := range ch {
                    result := work(input)
                    fmt.Println("pollWork :", result)
                }
            }()
        }

        cancel := func() {
            close(ch)
            wg.Wait()
        }

        return ch, cancel
    }

In the poolWork function, the same two generic types are declared to represent the input and return type for the work function. A WaitGroup is constructed to manage the lifecycle of the Goroutines in the pool. Then a channel is constructed of the generic Input type. This channel is used by the Goroutines in the pool to receive the input data for the work function.

Then the pool of Goroutines are created with each Goroutine waiting in a receive operation using a for-range loop against the channel. Finally, a cancel function is constructed to allow the caller to shutdown the pool and wait for all the Goroutines to signal they have terminated.

To test the use of the poolWork function, I built a second small program. 


Listing 19

    func main() {
        size := runtime.GOMAXPROCS(0)
        pwf := func(input int) string {
            time.Sleep(time.Duration(rand.Intn(200)) * time.Millisecond)
            return fmt.Sprintf("%d : received", input)
        }
    
        ch, cancel := poolWork(size, pwf)
        defer cancel()
    
        for i := 0; i < 4; i++ {
            ch <- i
        }
    }

Output:

    pollWork : 3 : received
    pollWork : 2 : received
    pollWork : 1 : received
    pollWork : 0 : received

The size of the pool is calculated based on the number of Goroutines that can run in parallel. Then a work function is constructed to sleep for a random amount of time and then return a string that represents the input.

With that in place, the poolWork function is executed and the channel and cancel function returned. The cancel function is deferred and a loop is constructed to send 4 values into the pool. The output will be different each time I run the program since this work is happening concurrently.

These little examples provide some insight into how a concurrent package could be implemented.

** Notes

- The work code provides a pattern for giving work to a set number of goroutines without losing the guarantee.
- The resource pooling code provides a pattern for managing resources that goroutines may need to acquire and release.
- The search code provides a pattern for using multiple goroutines to perform concurrent work.

** Links

- [[https://github.com/gobridge/concurrency-patterns][Concurrency patterns]]    
- [[https://blog.golang.org/pipelines][Go Concurrency Patterns: Pipelines and cancellation]] - Sameer Ajmani    
- [[https://talks.golang.org/2012/concurrency.slide#1][Go Concurrency Patterns]] - Rob Pike    
- [[https://blog.golang.org/context][Go Concurrency Patterns: Context]] - Sameer Ajmani    
- [[https://blog.golang.org/advanced-go-concurrency-patterns][Advanced Go Concurrency Patterns]] - Sameer Ajmani    
- [[https://talks.golang.org/2012/chat.slide][Go: code that grows with grace]] - Andrew Gerrand    

** Ardan Labs Kit

[[https://github.com/ardanlabs/kit][Open]]


All material is licensed under the [[http://www.apache.org/licenses/LICENSE-2.0][Apache License Version 2.0, January 2004]].
