Data Races
A data race is when two or more goroutines attempt to read and write to the same resource at the same time. Race conditions can create bugs that appear totally random or can never surface as they corrupt data.

* Data Races

A data race is when two or more Goroutines are trying to access the same memory location at the same time where at least one Goroutine is performing a write. When this happens it is impossible to predict the result. These types of bugs are difficult to find because they cause issues that always appear random.

These ~8 minutes from Scott Meyers is great to listen to here:

[[https://youtu.be/WDIkqP4JbkE?t=1809][CPU Caches and Why You Care 30:09-38:30]]

** Code Review

- *Example* *1:* Data Race
.play data_race/example1/example1.go
     

- *Example* *2:* Atomic Increments
.play data_race/example2/example2.go

- *Example* *3:* Mutex
.play data_race/example3/example3.go
    

- *Example* *4:* Read/Write Mutex
.play data_race/example4/example4.go
     

- *Example* *5:* Map Data Race
.play data_race/example5/example5.go
     
- *Example* *1:* Interface Based Race Condition
.play data_race/advanced/example1/example1.go
 
** Data Race Example
This is a great example of a data race and how they can be hidden for years and eventually show up at odd times and cause data corruption.

Listing 1

var counter int

    func main() {
        const grs = 2

        var wg sync.WaitGroup
        wg.Add(grs)

        for g := 0; g < grs; g++ {
            go func() {
                for i := 0; i < 2; i++ {
                    value := counter
                    value++
                    counter = value
                }
                wg.Done()
            }()
        }

        wg.Wait()
        fmt.Println("Counter:", counter)
    }

This program creates two Goroutines that each access the same integer variable, incrementing the variable twice. The Goroutine performs a read, modify, and write operation against the shared state manually.

Listing 2

    var counter int

    func main() {
        . . .

        go func() {
            for i := 0; i < 2; i++ {
                value := counter
                value++
                counter = value
            }
            wg.Done()
        }()

        . . .
    }

I can see the access to the shared state inside the for loop. When I build and run this program I get the right answer of 4 each and every time.

Listing 3

    $ ./example1
    Final Counter: 4

    $ ./example1
    Final Counter: 4

    $ ./example1
    Final Counter: 4


How is this working?


Listing 4

    G1                            Shared State: 0                            G2
    ----------------------------------------------------------------------------
    Read:   0
    Modify: 1
    Write:  1                         Shared State: 1
    Context Switch 
                                                                      Read: 1
                                                                    Modify: 2
                                    Shared State: 2                  Write: 2
                                                               Context Switch 
    Read:   2
    Modify: 3
    Write:  3                         Shared State: 3
    Terminate
                                                                      Read: 3
                                                                    Modify: 4
                                    Shared State: 4                  Write: 4
                                                                    Terminate
    ----------------------------------------------------------------------------

The read, modify and write operations are happening uninterrupted. Just because I am getting the right answer doesn’t mean there isn’t a problem. What happens if I add a log statement in the middle of the read, modify, and write operation?

Listing 5

    var counter int

    func main() {
        . . .

        go func() {
            for i := 0; i < 2; i++ {
                value := counter
                value++
                log.Println("logging")     <-- Add Logging Here
                counter = value
            }
            wg.Done()
        }()

        . . .
    }

If I run this program I no longer get the same result of 4, now I get the answer of 2.


Listing 6

    $ ./example1
    Final Counter: 2

    $ ./example1
    Final Counter: 2

    $ ./example1
    Final Counter: 2


What is happening? I am running into a data race bug that did exist before, but wasn’t happening. The call to log is now causing the scheduler to make a context switch between the two Goroutines at a bad time.

Listing 7

    G1                                Shared State: 0                         G2
    ----------------------------------------------------------------------------
    Read:   0
    Modify: 1
    Context Switch
                                                                        Read:   0
                                                                        Modify: 1
                                                                   Context Switch 
    Write:  1                         Shared State: 1
    Read:   1
    Modify: 2
    Context Switch
                                    Shared State: 1                     Write:  1
                                                                        Read:   1
                                                                        Modify: 2
                                                                    Context Switch 
    Write:  2                         Shared State: 2
    Terminate
                                    Shared State: 2                     Write:  2
                                                                        Terminate
    ----------------------------------------------------------------------------

After the modify operation a context switch is taking place. The three operations are no longer uninterrupted and Goroutine 2 ends up with its local value being wrong by the time it completes the write operation. I am very lucky this is happening every time and I can see it. But normally a data race like this happens "randomly" and is impossible to know about until it’s too late. Luckily Go has a race detector to help find data races.

** Race Detection
There are several ways to engage the race detector. I can use it with the run, build and test command. If I use it with the build command, I have to remember to run the program. They say an instrumented binary can slow my program down by ~20%.

Listing 8

    $ go build -race
    $ ./example1

The -race flag is how to instrument the build with the race detector. I will probably use it more with "go test", but for this example I am instrumenting the binary and then running it.

Listing 9

    2021/02/01 17:30:52 logging
    2021/02/01 17:30:52 logging
    2021/02/01 17:30:52 logging
    ==================
    WARNING: DATA RACE
    Write at 0x000001278d88 by goroutine 8:
    main.main.func1()
        /data_race/example1/example1.go:41 +0xa6

    Previous read at 0x000001278d88 by goroutine 7:
    main.main.func1()
        /data_race/example1/example1.go:38 +0x4a

    Goroutine 8 (running) created at:
    main.main()
        /data_race/example1/example1.go:36 +0xaf

    Goroutine 7 (finished) created at:
    main.main()
        /data_race/example1/example1.go:36 +0xaf
    ==================
    2021/02/01 17:30:52 logging
    Final Counter: 2
    Found 1 data race(s)

I can see a race was detected when running the program. This would happen with or without the log statement inserted. When a race is detected, the program panics and provides this trace. The trace shows where there was unsynchronized access to the same shared state where at least one access was a write.

In this trace, a Goroutine performed a write at address 0x000001278d88 on line 41, and there was an unsynchronized read at the same address by another Goroutine on line 38. Both Goroutines were created on line 36.


Listing 10

    36 go func() {
    37     for i := 0; i < 2; i++ {
    38         value := counter
    39         value++
    40         log.Println("logging")
    41         counter = value
    42     }
    43     wg.Done()
    44 }()

I can clearly see the unsynchronized read and write. As a side note, the plus plus operation on line 39 would also be a data race if the code was accessing the counter variable. The plus plus operation is a read, modify, and write operation underneath and the operating system could easily context switch in the middle of that.

So how can I fix the code to make sure that I remove the data race? There are two tools I can use, atomic instructions and mutexes.

** Atomics
Atomics provide synchronization at the hardware level. Because of this, it’s limited to words and half-words of data. So they’re great for counters or fast switching mechanics. The WaitGroup API’s use atomics.

What changes do I need to make to apply atomics to the code?

Listing 11

    var counter int32                             <-- CHANGED

    func main() {
        const grs = 2

        var wg sync.WaitGroup
        wg.Add(grs)

        for g := 0; g < grs; g++ {
            go func() {
                for i := 0; i < 2; i++ {
                    atomic.AddInt32(&counter, 1)  <-- CHANGED
                }
                wg.Done()
            }()
        }

        wg.Wait()
        fmt.Println("Counter:", counter)
    }

I only need to do a couple things. First, change the counter variable to be a precision based integer. I can see that at the top of the code listing. The atomic functions only work with precision based integers. Second, remove the manually read, modify, and write code for one call to atomic.AddInt32. That one call handles it all.

All of the functions associated with the atomic package take the address to the shared state to be synchronized. Synchronization only happens at the address level. So different Goroutines calling the same function, but at a different address, won’t be synchronized.

The API for atomics looks like this:

Listing 12

    func AddInt32(addr *int32, delta int32) (new int32)
    func AddInt64(addr *int64, delta int64) (new int64)
    func AddUint32(addr *uint32, delta uint32) (new uint32)
    func AddUint64(addr *uint64, delta uint64) (new uint64)
    func AddUintptr(addr *uintptr, delta uintptr) (new uintptr)

    func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool)
    func CompareAndSwapInt64(addr *int64, old, new int64) (swapped bool)
    func CompareAndSwapPointer(addr *unsafe.Pointer, old, new unsafe.Pointer) (swapped bool)
    func CompareAndSwapUint32(addr *uint32, old, new uint32) (swapped bool)
    func CompareAndSwapUint64(addr *uint64, old, new uint64) (swapped bool)
    func CompareAndSwapUintptr(addr *uintptr, old, new uintptr) (swapped bool)

    func LoadInt32(addr *int32) (val int32)
    func LoadInt64(addr *int64) (val int64)
    func LoadPointer(addr *unsafe.Pointer) (val unsafe.Pointer)
    func LoadUint32(addr *uint32) (val uint32)
    func LoadUint64(addr *uint64) (val uint64)
    func LoadUintptr(addr *uintptr) (val uintptr)

    func StoreInt32(addr *int32, val int32)
    func StoreInt64(addr *int64, val int64)
    func StorePointer(addr *unsafe.Pointer, val unsafe.Pointer)
    func StoreUint32(addr *uint32, val uint32)
    func StoreUint64(addr *uint64, val uint64)
    func StoreUintptr(addr *uintptr, val uintptr)

    func SwapInt32(addr *int32, new int32) (old int32)
    func SwapInt64(addr *int64, new int64) (old int64)
    func SwapPointer(addr *unsafe.Pointer, new unsafe.Pointer) (old unsafe.Pointer)
    func SwapUint32(addr *uint32, new uint32) (old uint32)
    func SwapUint64(addr *uint64, new uint64) (old uint64)
    func SwapUintptr(addr *uintptr, new uintptr) (old uintptr)

    type Value
        func (v *Value) Load() (x interface{})
        func (v *Value) Store(x interface{})

I can see that the first parameter is always the address to a precision based integer or pointer. There is also a type named Value that provides a synchronous value with a small API.

** Mutexes
What if I wanted to keep the three lines of code I had. Then atomics aren’t going to work. What I need then is a mutex. A mutex lets me box a group of code so only one Goroutine at a time can execute that code.

Listing 13

    var counter int

    func main() {
        const grs = 2

        var wg sync.WaitGroup
        wg.Add(grs)

        var mu sync.Mutex                    <-- CHANGED

        for g := 0; g < grs; g++ {
            go func() {
                for i := 0; i < 2; i++ {
                    mu.Lock()                <-- CHANGED
                    {
                        value := counter
                        value++
                        counter = value
                    }
                    mu.Unlock()              <-- CHANGED
                }
                wg.Done()
            }()
        }

        wg.Wait()
        fmt.Println("Counter:", counter)
    }

There are several changes to this code from the original. I added the construction of the mu variable to be a mutex set to its zero value. Then inside the for loop, I added calls to Lock and Unlock with an artificial code block. Inside the code block I have the code that needs to be synchronized. The code block is used for readability.

With this code in place, the scheduler will only allow one Goroutine to enter the code block at a time. It’s important to understand that a mutex is not a queue. The first Goroutine that calls Lock isn’t necessarily the first Goroutine who gets the Lock. There is a fairness based algorithm but this is done on purpose so people don’t use mutexes as queues.

It’s important to remember the Lock creates back pressure, so the longer it takes to get from the Lock to the Unlock, the more chance of Goroutines waiting for their turn. If I forget to call Unlock, then all Goroutines waiting will deadlock. This is why it’s critical that the call to Lock and Unlock happen in the same function. Make sure I’m doing the bare minimum synchronization I need in the code block, but at least the minimum.

This is very bad code where someone is trying to get in and out of the Lock so quickly they actually lose the synchronization and the race detector can’t even discover the problem.

Listing 14

    var counter int

    func main() {
        const grs = 2

        var wg sync.WaitGroup
        wg.Add(grs)

        var mu sync.Mutex

        for g := 0; g < grs; g++ {
            go func() {
                for i := 0; i < 2; i++ {
                    var value int
                    mu.Lock()              <-- Bad Use Of Mutex
                    {
                        value = counter
                    }
                    mu.Unlock()

                    value++

                    mu.Lock()              <-- Bad Use Of Mutex
                    {
                        counter = value
                    }
                    mu.Unlock()
                }
                wg.Done()
            }()
        }

        wg.Wait()
        fmt.Println("Counter:", counter)
    }

As a general guideline, if I see a call to Lock from the same mutex twice in the same function, stop the code review. There is probably a mistake or over complication. In this case the calls to read and write are being synchronized, however, two Goroutines can end up at the value++ line of code with the same value. The data race still exists and the race detector is helpless in finding it.

** Read/Write Mutexes
There is a second type of mutex called a read/write mutex. It allows me to separate the locks around reads and writes. This is important since reading data doesn’t pose a threat unless a Goroutine is attempting to write at the same time. So this type of mutex allows multiple Goroutines to read the same memory at the same time. As soon as a write lock is requested, the reads are no longer issued, the write takes place, the reads can start again.

Listing 15

    package main

    import (
        "fmt"
        "math/rand"
        "sync"
        "time"
    )

    var data []string
    var rwMutex sync.RWMutex

    func main() {
        var wg sync.WaitGroup
        wg.Add(1)

        go func() {
            for i := 0; i < 10; i++ {
                writer(i)
            }
            wg.Done()
        }()

        for i := 0; i < 8; i++ {
            go func(id int) {
                for {
                    reader(id)
                }
            }(i)
        }

        wg.Wait()
        fmt.Println("Program Complete")
    }

    func writer(i int) {
        rwMutex.Lock()
        {
            time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)
            fmt.Println("****> : Performing Write")
            data = append(data, fmt.Sprintf("String: %d", i))
        }
        rwMutex.Unlock()
    }

    func reader(id int) {
        rwMutex.RLock()
        {
            time.Sleep(time.Duration(rand.Intn(10)) * time.Millisecond)
            fmt.Printf("%d : Performing Read : Length[%d]\n", id, len(data))
        }
        rwMutex.RUnlock()
    }

I can see the use of a read/write mutex where there are 8 Goroutines reading the length of a slice within a 10 millisecond delay of each other, and 1 Goroutine waking up within 100 milliseconds to append a value (write) to the slice.

The key is the implementation of the writer and reader functions. Notice how I use Lock for the writer and RLock for the reader. One of the biggest mistakes I can make with this is mixing up the Unlock calls with the wrong version. Having a Lock with a RUnlock will never end well.

Listing 16

    7 : Performing Read : Length[0]
    5 : Performing Read : Length[0]
    0 : Performing Read : Length[0]
    3 : Performing Read : Length[0]
    7 : Performing Read : Length[0]
    2 : Performing Read : Length[0]
    1 : Performing Read : Length[0]
    ****> : Performing Write
    0 : Performing Read : Length[1]
    5 : Performing Read : Length[1]
    3 : Performing Read : Length[1]
    6 : Performing Read : Length[1]
    7 : Performing Read : Length[1]
    4 : Performing Read : Length[1]
    1 : Performing Read : Length[1]
    2 : Performing Read : Length[1]
    ****> : Performing Write
    7 : Performing Read : Length[2]
    1 : Performing Read : Length[2]
    3 : Performing Read : Length[2]

The output shows how multiple Goroutines are reading at the same time, but all the reading stops when the write takes place.

** Notes

- Goroutines need to be coordinated and synchronized.
- When two or more goroutines attempt to access the same resource, we have a data race.
- Atomic functions and mutexes can provide the support we need.

** Cache Coherency and False Sharing
This content is provided by Scott Meyers from his talk in 2014 at Dive:

[[https://youtu.be/WDIkqP4JbkE?t=1809][CPU Caches and Why You Care (30:09-38:30)]]  
[[https://github.com/ardanlabs/gotraining/blob/master/topics/go/testing/benchmarks/falseshare/README.md][Code Example]]

.image /tour/static/img/figure1_data_race.png

** Cache Coherency and False Sharing Notes

- Thread memory access matters.
- If your algorithm is not scaling look for false sharing problems.

** Links

- [[http://www.drdobbs.com/parallel/eliminate-false-sharing/217500206][Eliminate False Sharing]] - Herb Sutter    
- [[https://golang.org/ref/mem][The Go Memory Model]]    
- [[http://blog.golang.org/race-detector][Introducing the Go Race Detector]] - Dmitry Vyukov and Andrew Gerrand    
- [[https://www.ardanlabs.com/blog/2013/09/detecting-race-conditions-with-go.html][Detecting Race Conditions With Go]] - William Kennedy    
- [[https://golang.org/doc/articles/race_detector.html][Data Race Detector]]    

** Diagram

View of Data Race in Example1.

.image /tour/static/img/data_race.png


* Exercises

Exercise 1

Given the following program, use the race detector to find and correct the data race.

.play data_race/exercises/template1/template1.go

.play data_race/exercises/exercise1/exercise1.go
 

All material is licensed under the [[http://www.apache.org/licenses/LICENSE-2.0][Apache License Version 2.0, January 2004]].
